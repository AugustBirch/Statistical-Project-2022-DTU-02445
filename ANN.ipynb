{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-3f4f72cf-66f0-4ae1-98df-12e684665477",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f4f4a1ce",
    "execution_start": 1642514965829,
    "execution_millis": 2254,
    "deepnote_output_heights": [
     194.03125
    ],
    "deepnote_cell_type": "code"
   },
   "source": "import pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nimport torch \nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom scipy.stats import binom\nfrom statsmodels.stats.contingency_tables import mcnemar\nimport scipy.stats\nimport scipy.stats as st\n\n# from KNNCT.ipynb import rightWrong_DT, rightWrong_KNN",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ee50dc6f-14d1-46c8-bbd0-afb8bd03ec0e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e7e48698",
    "execution_start": 1642160201858,
    "execution_millis": 13863,
    "deepnote_output_heights": [
     null,
     405.1875
    ],
    "deepnote_cell_type": "code"
   },
   "source": "df = pd.read_csv('armdatadf.csv')\ndf['setup'] = df['setup'].astype(int)\nX = df[[\"x\",\"y\",\"z\"]]\nY = df[\"setup\"]\nfacit = Y[::100].values\nX.x = (X.x-X.x.mean()) / X.x.std()\nX.y = (X.y-X.y.mean()) / X.y.std()\nX.z = (X.z-X.z.mean()) / X.z.std()\nXM = np.empty([300])\n\nfor i in range(1600):\n    XM = np.vstack([XM,X[[\"x\",\"y\",\"z\"]][i*100:(i+1)*100].values.ravel()])\n\nXM = np.delete(XM,0,0)\n\nprint(XM.shape)\n\nXM_XY  = np.empty([200])\nXM_XZ  = np.empty([200])\nXM_YZ  = np.empty([200])\n\nfor i in range(1600):\n    XM_XY = np.vstack([XM_XY,X[[\"x\",\"y\"]][i*100:(i+1)*100].values.ravel()])\n    XM_XZ = np.vstack([XM_XZ,X[[\"x\",\"z\"]][i*100:(i+1)*100].values.ravel()])\n    XM_YZ = np.vstack([XM_YZ,X[[\"y\",\"z\"]][i*100:(i+1)*100].values.ravel()])\nXM_XY = np.delete(XM_XY,0,0)\nXM_XZ = np.delete(XM_XZ,0,0)\nXM_YZ = np.delete(XM_YZ,0,0)\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self[name] = value\n(1600, 300)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c8568d71-c429-4cfd-bef3-e8fba1851f6d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c3062c4",
    "execution_start": 1642160335707,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "###################################################################################################################\ndef ANNTrain(X_train, y_train, h=25, Batch_size=25, number_of_training_iterations_ANN=4000):    \n    #hyperparameters\n    n_inputs = 300\n    n_outputs = 16\n    #Single hidden layer neural network with h hidden neurons\n    model = torch.nn.Sequential(\n        torch.nn.Linear(n_inputs, h),\n        torch.nn.Tanh(),\n        torch.nn.Linear(h, n_outputs),\n        torch.nn.Softmax()\n        )\n\n    #defining loss type\n    loss_fn = torch.nn.MSELoss(reduction='sum') \n    X_train = torch.from_numpy(X_train).to(torch.float32)\n    y_train = torch.from_numpy(y_train).to(torch.float32)\n    learning_rate = 0.005\n\n\n    for t in range(number_of_training_iterations_ANN):\n        indexes = np.random.randint(1440, size=Batch_size) #for every person in the X_train dataset, any setup and any repetition. \n        x = torch.tensor(X_train[indexes]).to(torch.float32)\n        \n        y = torch.tensor(y_train[indexes])\n        y = y.to(torch.int64).t().squeeze()\n        y = F.one_hot(y, num_classes=16).to(torch.float32)\n        y_pred = model(x)\n        # print(y_pred.shape)\n        loss = loss_fn(y_pred, y)\n        \n        #delete the following after model is tested\n        #if t % 100 == 99:\n            #print(loss)\n\n        model.zero_grad()\n        #computing gradients\n        loss.backward()\n\n        #update parameters with the computed gradients\n        with torch.no_grad():\n            for param in model.parameters():\n                param -= learning_rate * param.grad\n    return model\n\ndef ANNTest(X_test, y_test, model):\n    #y_test = torch.from_numpy(y_test).to(torch.float32) \n    \n    # y = torch.tensor(y_test)\n    # y = y.to(torch.int64).t().squeeze()\n\n    errors = 0\n    y = torch.tensor(y_test)\n    y = F.one_hot(y, num_classes=16).to(torch.float32)\n\n    x = torch.from_numpy(X_test).to(torch.float32)\n    \n    rightWrongPerson = np.zeros(10*16)\n    for i in range(16*10):\n        with torch.no_grad():\n            inp = x[i]\n            output = model(inp)\n\n            predIndex = torch.argmax(output)\n            \n            prediction = torch.zeros([16])\n            prediction[predIndex] = 1\n\n            middleStep = y[i]-prediction\n            errors += torch.max(middleStep).item()\n\n            if (torch.max(middleStep).item()==0):\n                rightWrongPerson[i] = 1\n\n    return rightWrongPerson",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d60d92cf-e9c2-4cbf-971e-3fc34746986c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9861a6f5",
    "execution_start": 1642160334446,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "def ANNTrain2Var(X_train, y_train, h=25, Batch_size=25, number_of_training_iterations_ANN=4000):    \n    #hyperparameters\n    n_inputs = 200\n    n_outputs = 16\n    #Single hidden layer neural network with h hidden neurons\n    model = torch.nn.Sequential(\n        torch.nn.Linear(n_inputs, h),\n        torch.nn.Tanh(),\n        torch.nn.Linear(h, n_outputs),\n        torch.nn.Softmax()\n        )\n\n    #defining loss type\n    loss_fn = torch.nn.MSELoss(reduction='sum') \n    X_train = torch.from_numpy(X_train).to(torch.float32)\n    y_train = torch.from_numpy(y_train).to(torch.float32)\n    learning_rate = 0.005\n\n\n    for t in range(number_of_training_iterations_ANN):\n        indexes = np.random.randint(1440, size=Batch_size) #for every person in the X_train dataset, any setup and any repetition. \n        x = torch.tensor(X_train[indexes]).to(torch.float32)\n        \n        y = torch.tensor(y_train[indexes])\n        y = y.to(torch.int64).t().squeeze()\n        y = F.one_hot(y, num_classes=16).to(torch.float32)\n        y_pred = model(x)\n        # print(y_pred.shape)\n        loss = loss_fn(y_pred, y)\n        \n        #delete the following after model is tested\n        #if t % 100 == 99:\n            #print(loss)\n\n        model.zero_grad()\n        #computing gradients\n        loss.backward()\n\n        #update parameters with the computed gradients\n        with torch.no_grad():\n            for param in model.parameters():\n                param -= learning_rate * param.grad\n    return model\n\ndef ANNTest2Var(X_test, y_test, model):\n    #y_test = torch.from_numpy(y_test).to(torch.float32) \n    \n    # y = torch.tensor(y_test)\n    # y = y.to(torch.int64).t().squeeze()\n\n    errors = 0\n    y = torch.tensor(y_test)\n    y = F.one_hot(y, num_classes=16).to(torch.float32)\n\n    x = torch.from_numpy(X_test).to(torch.float32)\n    \n    rightWrongPerson = np.zeros(10*16)\n    for i in range(16*10):\n        with torch.no_grad():\n            inp = x[i]\n            output = model(inp)\n\n            predIndex = torch.argmax(output)\n            \n            prediction = torch.zeros([16])\n            prediction[predIndex] = 1\n\n            middleStep = y[i]-prediction\n            errors += torch.max(middleStep).item()\n\n            if (torch.max(middleStep).item()==0):\n                rightWrongPerson[i] = 1\n\n    return rightWrongPerson\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "083d84e1-16af-4865-bb02-1b8444406a66",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af7cc6d3",
    "execution_start": 1642160621698,
    "execution_millis": 187473,
    "deepnote_cell_type": "code"
   },
   "source": "#######################################################################################\n\nrightWrongXYZ = np.zeros((10,10*16))\nrightWrongXY  = np.zeros((10,10*16))\nrightWrongXZ  = np.zeros((10,10*16))\nrightWrongYZ  = np.zeros((10,10*16))\n\nfor i in range(10):\n    test_IDS = list(range(160*i,160*(i+1)))\n\n    y_test   = facit[test_IDS]\n    y_train  = np.delete(facit,test_IDS)\n    y_train  = y_train-1\n    y_test   = y_test-1\n\n    #Training and testing site\n    X_test   = XM[test_IDS]\n    X_train  = np.delete(XM, test_IDS, axis = 0)\n    model = ANNTrain(X_train,y_train)\n    rightWrongXYZ[i] = ANNTest(X_test, y_test, model)\n    \n    X_test   = XM_XY[test_IDS]\n    X_train  = np.delete(XM_XY, test_IDS, axis = 0)\n    model = ANNTrain2Var(X_train,y_train)\n    rightWrongXY[i] = ANNTest2Var(X_test, y_test, model)\n\n    X_test   = XM_XZ[test_IDS]\n    X_train  = np.delete(XM_XZ, test_IDS, axis = 0)\n    model = ANNTrain2Var(X_train,y_train)\n    rightWrongXZ[i] = ANNTest2Var(X_test, y_test, model)\n\n    X_test   = XM_YZ[test_IDS]\n    X_train  = np.delete(XM_YZ, test_IDS, axis = 0)\n    model = ANNTrain2Var(X_train,y_train)\n    rightWrongYZ[i] = ANNTest2Var(X_test, y_test, model)\n\n    #the returned generalization error when testing   (number of correctly classified setups in test)/(length of trajectories in test)\n\nnumber_of_true  = (rightWrongXYZ == 1).sum() #counts in all dimensions\nnumber_of_wrong = (rightWrongXYZ == 0).sum() #counts in all dimensions\nprint(number_of_true/(number_of_true+number_of_wrong))\nnumber_of_true  = (rightWrongXY == 1).sum() #counts in all dimensions\nnumber_of_wrong = (rightWrongXY == 0).sum() #counts in all dimensions\nprint(number_of_true/(number_of_true+number_of_wrong))\nnumber_of_true  = (rightWrongXZ == 1).sum() #counts in all dimensions\nnumber_of_wrong = (rightWrongXZ == 0).sum() #counts in all dimensions\nprint(number_of_true/(number_of_true+number_of_wrong))\nnumber_of_true  = (rightWrongYZ == 1).sum() #counts in all dimensions\nnumber_of_wrong = (rightWrongYZ == 0).sum() #counts in all dimensions\nprint(number_of_true/(number_of_true+number_of_wrong))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "tensor(20.2749, grad_fn=<MseLossBackward0>)\ntensor(17.8261, grad_fn=<MseLossBackward0>)\ntensor(16.7220, grad_fn=<MseLossBackward0>)\ntensor(18.1534, grad_fn=<MseLossBackward0>)\ntensor(18.7981, grad_fn=<MseLossBackward0>)\ntensor(17.0758, grad_fn=<MseLossBackward0>)\ntensor(18.3397, grad_fn=<MseLossBackward0>)\ntensor(16.3785, grad_fn=<MseLossBackward0>)\ntensor(16.4514, grad_fn=<MseLossBackward0>)\ntensor(16.2803, grad_fn=<MseLossBackward0>)\ntensor(17.4049, grad_fn=<MseLossBackward0>)\ntensor(16.6649, grad_fn=<MseLossBackward0>)\ntensor(14.6231, grad_fn=<MseLossBackward0>)\ntensor(18.3008, grad_fn=<MseLossBackward0>)\ntensor(15.2463, grad_fn=<MseLossBackward0>)\ntensor(15.3943, grad_fn=<MseLossBackward0>)\ntensor(16.3608, grad_fn=<MseLossBackward0>)\ntensor(15.1797, grad_fn=<MseLossBackward0>)\ntensor(16.9877, grad_fn=<MseLossBackward0>)\ntensor(14.2626, grad_fn=<MseLossBackward0>)\ntensor(14.6033, grad_fn=<MseLossBackward0>)\ntensor(21.1520, grad_fn=<MseLossBackward0>)\ntensor(12.4714, grad_fn=<MseLossBackward0>)\ntensor(17.5467, grad_fn=<MseLossBackward0>)\ntensor(12.8447, grad_fn=<MseLossBackward0>)\ntensor(15.8156, grad_fn=<MseLossBackward0>)\ntensor(17.8452, grad_fn=<MseLossBackward0>)\ntensor(15.5269, grad_fn=<MseLossBackward0>)\ntensor(19.6100, grad_fn=<MseLossBackward0>)\ntensor(15.2793, grad_fn=<MseLossBackward0>)\ntensor(14.2660, grad_fn=<MseLossBackward0>)\ntensor(15.4385, grad_fn=<MseLossBackward0>)\ntensor(13.9233, grad_fn=<MseLossBackward0>)\ntensor(14.8564, grad_fn=<MseLossBackward0>)\ntensor(16.1987, grad_fn=<MseLossBackward0>)\ntensor(22.6404, grad_fn=<MseLossBackward0>)\ntensor(20.9986, grad_fn=<MseLossBackward0>)\ntensor(19.5967, grad_fn=<MseLossBackward0>)\ntensor(18.4092, grad_fn=<MseLossBackward0>)\ntensor(16.5243, grad_fn=<MseLossBackward0>)\ntensor(16.4443, grad_fn=<MseLossBackward0>)\ntensor(16.8960, grad_fn=<MseLossBackward0>)\ntensor(15.2348, grad_fn=<MseLossBackward0>)\ntensor(16.2849, grad_fn=<MseLossBackward0>)\ntensor(15.6031, grad_fn=<MseLossBackward0>)\ntensor(18.1696, grad_fn=<MseLossBackward0>)\ntensor(15.9914, grad_fn=<MseLossBackward0>)\ntensor(17.8521, grad_fn=<MseLossBackward0>)\ntensor(18.9871, grad_fn=<MseLossBackward0>)\ntensor(14.9624, grad_fn=<MseLossBackward0>)\ntensor(15.1316, grad_fn=<MseLossBackward0>)\ntensor(12.6553, grad_fn=<MseLossBackward0>)\ntensor(13.1758, grad_fn=<MseLossBackward0>)\ntensor(15.8206, grad_fn=<MseLossBackward0>)\ntensor(11.7121, grad_fn=<MseLossBackward0>)\ntensor(13.1053, grad_fn=<MseLossBackward0>)\ntensor(14.0733, grad_fn=<MseLossBackward0>)\ntensor(13.7994, grad_fn=<MseLossBackward0>)\ntensor(14.3550, grad_fn=<MseLossBackward0>)\ntensor(15.9839, grad_fn=<MseLossBackward0>)\ntensor(17.4541, grad_fn=<MseLossBackward0>)\ntensor(16.1263, grad_fn=<MseLossBackward0>)\ntensor(13.9032, grad_fn=<MseLossBackward0>)\ntensor(13.7385, grad_fn=<MseLossBackward0>)\ntensor(12.0727, grad_fn=<MseLossBackward0>)\ntensor(13.5687, grad_fn=<MseLossBackward0>)\ntensor(12.2186, grad_fn=<MseLossBackward0>)\ntensor(12.8429, grad_fn=<MseLossBackward0>)\ntensor(13.8360, grad_fn=<MseLossBackward0>)\ntensor(14.3870, grad_fn=<MseLossBackward0>)\ntensor(11.3101, grad_fn=<MseLossBackward0>)\ntensor(12.9406, grad_fn=<MseLossBackward0>)\ntensor(13.2505, grad_fn=<MseLossBackward0>)\ntensor(11.8365, grad_fn=<MseLossBackward0>)\ntensor(17.0211, grad_fn=<MseLossBackward0>)\ntensor(21.8507, grad_fn=<MseLossBackward0>)\ntensor(20.7461, grad_fn=<MseLossBackward0>)\ntensor(21.6870, grad_fn=<MseLossBackward0>)\ntensor(22.3752, grad_fn=<MseLossBackward0>)\ntensor(19.0190, grad_fn=<MseLossBackward0>)\ntensor(22.3682, grad_fn=<MseLossBackward0>)\ntensor(21.0731, grad_fn=<MseLossBackward0>)\ntensor(21.1963, grad_fn=<MseLossBackward0>)\ntensor(21.7535, grad_fn=<MseLossBackward0>)\ntensor(19.1812, grad_fn=<MseLossBackward0>)\ntensor(20.0384, grad_fn=<MseLossBackward0>)\ntensor(20.6564, grad_fn=<MseLossBackward0>)\ntensor(21.2714, grad_fn=<MseLossBackward0>)\ntensor(18.8196, grad_fn=<MseLossBackward0>)\ntensor(19.7148, grad_fn=<MseLossBackward0>)\ntensor(21.2052, grad_fn=<MseLossBackward0>)\ntensor(17.8492, grad_fn=<MseLossBackward0>)\ntensor(18.3936, grad_fn=<MseLossBackward0>)\ntensor(19.5575, grad_fn=<MseLossBackward0>)\ntensor(18.6624, grad_fn=<MseLossBackward0>)\ntensor(20.1551, grad_fn=<MseLossBackward0>)\ntensor(17.4873, grad_fn=<MseLossBackward0>)\ntensor(18.7784, grad_fn=<MseLossBackward0>)\ntensor(17.1549, grad_fn=<MseLossBackward0>)\ntensor(20.6300, grad_fn=<MseLossBackward0>)\ntensor(17.0396, grad_fn=<MseLossBackward0>)\ntensor(18.9767, grad_fn=<MseLossBackward0>)\ntensor(19.9464, grad_fn=<MseLossBackward0>)\ntensor(17.5555, grad_fn=<MseLossBackward0>)\ntensor(19.6053, grad_fn=<MseLossBackward0>)\ntensor(21.1855, grad_fn=<MseLossBackward0>)\ntensor(18.1051, grad_fn=<MseLossBackward0>)\ntensor(18.6540, grad_fn=<MseLossBackward0>)\ntensor(19.3623, grad_fn=<MseLossBackward0>)\ntensor(17.7683, grad_fn=<MseLossBackward0>)\ntensor(19.0590, grad_fn=<MseLossBackward0>)\ntensor(18.8898, grad_fn=<MseLossBackward0>)\ntensor(16.4655, grad_fn=<MseLossBackward0>)\ntensor(20.7851, grad_fn=<MseLossBackward0>)\ntensor(17.3093, grad_fn=<MseLossBackward0>)\ntensor(23.2229, grad_fn=<MseLossBackward0>)\ntensor(22.2652, grad_fn=<MseLossBackward0>)\ntensor(22.5140, grad_fn=<MseLossBackward0>)\ntensor(19.8778, grad_fn=<MseLossBackward0>)\ntensor(19.3550, grad_fn=<MseLossBackward0>)\ntensor(20.2139, grad_fn=<MseLossBackward0>)\ntensor(17.4806, grad_fn=<MseLossBackward0>)\ntensor(18.8843, grad_fn=<MseLossBackward0>)\ntensor(16.0939, grad_fn=<MseLossBackward0>)\ntensor(18.7925, grad_fn=<MseLossBackward0>)\ntensor(19.4353, grad_fn=<MseLossBackward0>)\ntensor(17.1778, grad_fn=<MseLossBackward0>)\ntensor(16.0362, grad_fn=<MseLossBackward0>)\ntensor(18.4094, grad_fn=<MseLossBackward0>)\ntensor(19.5696, grad_fn=<MseLossBackward0>)\ntensor(16.9294, grad_fn=<MseLossBackward0>)\ntensor(15.5643, grad_fn=<MseLossBackward0>)\ntensor(17.9637, grad_fn=<MseLossBackward0>)\ntensor(16.5494, grad_fn=<MseLossBackward0>)\ntensor(13.3769, grad_fn=<MseLossBackward0>)\ntensor(14.9871, grad_fn=<MseLossBackward0>)\ntensor(12.9418, grad_fn=<MseLossBackward0>)\ntensor(16.7865, grad_fn=<MseLossBackward0>)\ntensor(13.5208, grad_fn=<MseLossBackward0>)\ntensor(16.4826, grad_fn=<MseLossBackward0>)\ntensor(18.1518, grad_fn=<MseLossBackward0>)\ntensor(16.1498, grad_fn=<MseLossBackward0>)\ntensor(19.4219, grad_fn=<MseLossBackward0>)\ntensor(16.8130, grad_fn=<MseLossBackward0>)\ntensor(17.9727, grad_fn=<MseLossBackward0>)\ntensor(13.9124, grad_fn=<MseLossBackward0>)\ntensor(13.3749, grad_fn=<MseLossBackward0>)\ntensor(14.2892, grad_fn=<MseLossBackward0>)\ntensor(19.7764, grad_fn=<MseLossBackward0>)\ntensor(13.6367, grad_fn=<MseLossBackward0>)\ntensor(14.9943, grad_fn=<MseLossBackward0>)\ntensor(17.7470, grad_fn=<MseLossBackward0>)\ntensor(14.1886, grad_fn=<MseLossBackward0>)\ntensor(16.8299, grad_fn=<MseLossBackward0>)\ntensor(16.8411, grad_fn=<MseLossBackward0>)\ntensor(22.9056, grad_fn=<MseLossBackward0>)\ntensor(21.4874, grad_fn=<MseLossBackward0>)\ntensor(22.8627, grad_fn=<MseLossBackward0>)\ntensor(19.3225, grad_fn=<MseLossBackward0>)\ntensor(18.8277, grad_fn=<MseLossBackward0>)\ntensor(18.0765, grad_fn=<MseLossBackward0>)\ntensor(16.3097, grad_fn=<MseLossBackward0>)\ntensor(18.3734, grad_fn=<MseLossBackward0>)\ntensor(18.4009, grad_fn=<MseLossBackward0>)\ntensor(19.5913, grad_fn=<MseLossBackward0>)\ntensor(18.3081, grad_fn=<MseLossBackward0>)\ntensor(18.6047, grad_fn=<MseLossBackward0>)\ntensor(16.3973, grad_fn=<MseLossBackward0>)\ntensor(16.7499, grad_fn=<MseLossBackward0>)\ntensor(14.2609, grad_fn=<MseLossBackward0>)\ntensor(18.0295, grad_fn=<MseLossBackward0>)\ntensor(15.2740, grad_fn=<MseLossBackward0>)\ntensor(15.2775, grad_fn=<MseLossBackward0>)\ntensor(13.4770, grad_fn=<MseLossBackward0>)\ntensor(14.2896, grad_fn=<MseLossBackward0>)\ntensor(16.2092, grad_fn=<MseLossBackward0>)\ntensor(15.8946, grad_fn=<MseLossBackward0>)\ntensor(16.7005, grad_fn=<MseLossBackward0>)\ntensor(15.8176, grad_fn=<MseLossBackward0>)\ntensor(17.1988, grad_fn=<MseLossBackward0>)\ntensor(16.7766, grad_fn=<MseLossBackward0>)\ntensor(19.0289, grad_fn=<MseLossBackward0>)\ntensor(18.2443, grad_fn=<MseLossBackward0>)\ntensor(15.6431, grad_fn=<MseLossBackward0>)\ntensor(18.4162, grad_fn=<MseLossBackward0>)\ntensor(16.4828, grad_fn=<MseLossBackward0>)\ntensor(15.7748, grad_fn=<MseLossBackward0>)\ntensor(17.5356, grad_fn=<MseLossBackward0>)\ntensor(18.1357, grad_fn=<MseLossBackward0>)\ntensor(14.8009, grad_fn=<MseLossBackward0>)\ntensor(16.3119, grad_fn=<MseLossBackward0>)\ntensor(15.3618, grad_fn=<MseLossBackward0>)\ntensor(18.7010, grad_fn=<MseLossBackward0>)\ntensor(17.1099, grad_fn=<MseLossBackward0>)\ntensor(18.0840, grad_fn=<MseLossBackward0>)\ntensor(21.2518, grad_fn=<MseLossBackward0>)\ntensor(21.3984, grad_fn=<MseLossBackward0>)\ntensor(17.8492, grad_fn=<MseLossBackward0>)\ntensor(19.4379, grad_fn=<MseLossBackward0>)\ntensor(18.3793, grad_fn=<MseLossBackward0>)\ntensor(17.3164, grad_fn=<MseLossBackward0>)\ntensor(18.8991, grad_fn=<MseLossBackward0>)\ntensor(17.3237, grad_fn=<MseLossBackward0>)\ntensor(16.7565, grad_fn=<MseLossBackward0>)\ntensor(13.5902, grad_fn=<MseLossBackward0>)\ntensor(16.7322, grad_fn=<MseLossBackward0>)\ntensor(15.0625, grad_fn=<MseLossBackward0>)\ntensor(12.3898, grad_fn=<MseLossBackward0>)\ntensor(15.7797, grad_fn=<MseLossBackward0>)\ntensor(15.7192, grad_fn=<MseLossBackward0>)\ntensor(16.5568, grad_fn=<MseLossBackward0>)\ntensor(16.5737, grad_fn=<MseLossBackward0>)\ntensor(16.6233, grad_fn=<MseLossBackward0>)\ntensor(14.5790, grad_fn=<MseLossBackward0>)\ntensor(15.0992, grad_fn=<MseLossBackward0>)\ntensor(13.5227, grad_fn=<MseLossBackward0>)\ntensor(15.1852, grad_fn=<MseLossBackward0>)\ntensor(14.5592, grad_fn=<MseLossBackward0>)\ntensor(12.4957, grad_fn=<MseLossBackward0>)\ntensor(15.1993, grad_fn=<MseLossBackward0>)\ntensor(17.5085, grad_fn=<MseLossBackward0>)\ntensor(17.1523, grad_fn=<MseLossBackward0>)\ntensor(15.8282, grad_fn=<MseLossBackward0>)\ntensor(14.5842, grad_fn=<MseLossBackward0>)\ntensor(12.8396, grad_fn=<MseLossBackward0>)\ntensor(13.5666, grad_fn=<MseLossBackward0>)\ntensor(12.6171, grad_fn=<MseLossBackward0>)\ntensor(14.5735, grad_fn=<MseLossBackward0>)\ntensor(13.4408, grad_fn=<MseLossBackward0>)\ntensor(16.4163, grad_fn=<MseLossBackward0>)\ntensor(16.0987, grad_fn=<MseLossBackward0>)\ntensor(11.1076, grad_fn=<MseLossBackward0>)\ntensor(12.1338, grad_fn=<MseLossBackward0>)\ntensor(12.9983, grad_fn=<MseLossBackward0>)\ntensor(10.6559, grad_fn=<MseLossBackward0>)\ntensor(22.3664, grad_fn=<MseLossBackward0>)\ntensor(21.9481, grad_fn=<MseLossBackward0>)\ntensor(22.4543, grad_fn=<MseLossBackward0>)\ntensor(22.1987, grad_fn=<MseLossBackward0>)\ntensor(21.4584, grad_fn=<MseLossBackward0>)\ntensor(20.8759, grad_fn=<MseLossBackward0>)\ntensor(21.9395, grad_fn=<MseLossBackward0>)\ntensor(21.1838, grad_fn=<MseLossBackward0>)\ntensor(21.6707, grad_fn=<MseLossBackward0>)\ntensor(20.9948, grad_fn=<MseLossBackward0>)\ntensor(20.8735, grad_fn=<MseLossBackward0>)\ntensor(19.6031, grad_fn=<MseLossBackward0>)\ntensor(19.5500, grad_fn=<MseLossBackward0>)\ntensor(20.0996, grad_fn=<MseLossBackward0>)\ntensor(22.0228, grad_fn=<MseLossBackward0>)\ntensor(20.9459, grad_fn=<MseLossBackward0>)\ntensor(21.3080, grad_fn=<MseLossBackward0>)\ntensor(19.2455, grad_fn=<MseLossBackward0>)\ntensor(19.5348, grad_fn=<MseLossBackward0>)\ntensor(18.1910, grad_fn=<MseLossBackward0>)\ntensor(21.8010, grad_fn=<MseLossBackward0>)\ntensor(20.6651, grad_fn=<MseLossBackward0>)\ntensor(18.9045, grad_fn=<MseLossBackward0>)\ntensor(20.1258, grad_fn=<MseLossBackward0>)\ntensor(19.3980, grad_fn=<MseLossBackward0>)\ntensor(20.0915, grad_fn=<MseLossBackward0>)\ntensor(19.1324, grad_fn=<MseLossBackward0>)\ntensor(18.8613, grad_fn=<MseLossBackward0>)\ntensor(22.0261, grad_fn=<MseLossBackward0>)\ntensor(20.0576, grad_fn=<MseLossBackward0>)\ntensor(21.2948, grad_fn=<MseLossBackward0>)\ntensor(20.8309, grad_fn=<MseLossBackward0>)\ntensor(21.5045, grad_fn=<MseLossBackward0>)\ntensor(17.4611, grad_fn=<MseLossBackward0>)\ntensor(19.6129, grad_fn=<MseLossBackward0>)\ntensor(16.4683, grad_fn=<MseLossBackward0>)\ntensor(20.4984, grad_fn=<MseLossBackward0>)\ntensor(19.5943, grad_fn=<MseLossBackward0>)\ntensor(15.9328, grad_fn=<MseLossBackward0>)\ntensor(20.0132, grad_fn=<MseLossBackward0>)\ntensor(21.7921, grad_fn=<MseLossBackward0>)\ntensor(20.8339, grad_fn=<MseLossBackward0>)\ntensor(22.4785, grad_fn=<MseLossBackward0>)\ntensor(20.9315, grad_fn=<MseLossBackward0>)\ntensor(20.9621, grad_fn=<MseLossBackward0>)\ntensor(19.4897, grad_fn=<MseLossBackward0>)\ntensor(18.9928, grad_fn=<MseLossBackward0>)\ntensor(20.9333, grad_fn=<MseLossBackward0>)\ntensor(19.4538, grad_fn=<MseLossBackward0>)\ntensor(18.6891, grad_fn=<MseLossBackward0>)\ntensor(19.3832, grad_fn=<MseLossBackward0>)\ntensor(19.6290, grad_fn=<MseLossBackward0>)\ntensor(19.3932, grad_fn=<MseLossBackward0>)\ntensor(17.7510, grad_fn=<MseLossBackward0>)\ntensor(15.8941, grad_fn=<MseLossBackward0>)\ntensor(17.2552, grad_fn=<MseLossBackward0>)\ntensor(18.8700, grad_fn=<MseLossBackward0>)\ntensor(13.7608, grad_fn=<MseLossBackward0>)\ntensor(16.1577, grad_fn=<MseLossBackward0>)\ntensor(15.9886, grad_fn=<MseLossBackward0>)\ntensor(14.5775, grad_fn=<MseLossBackward0>)\ntensor(19.0726, grad_fn=<MseLossBackward0>)\ntensor(13.3168, grad_fn=<MseLossBackward0>)\ntensor(16.7600, grad_fn=<MseLossBackward0>)\ntensor(18.7446, grad_fn=<MseLossBackward0>)\ntensor(16.4127, grad_fn=<MseLossBackward0>)\ntensor(17.8555, grad_fn=<MseLossBackward0>)\ntensor(15.0151, grad_fn=<MseLossBackward0>)\ntensor(17.0374, grad_fn=<MseLossBackward0>)\ntensor(18.7793, grad_fn=<MseLossBackward0>)\ntensor(17.1349, grad_fn=<MseLossBackward0>)\ntensor(15.0510, grad_fn=<MseLossBackward0>)\ntensor(17.4802, grad_fn=<MseLossBackward0>)\ntensor(16.1736, grad_fn=<MseLossBackward0>)\ntensor(14.0576, grad_fn=<MseLossBackward0>)\ntensor(12.4739, grad_fn=<MseLossBackward0>)\ntensor(15.1532, grad_fn=<MseLossBackward0>)\ntensor(15.7724, grad_fn=<MseLossBackward0>)\ntensor(18.0098, grad_fn=<MseLossBackward0>)\ntensor(15.9292, grad_fn=<MseLossBackward0>)\ntensor(21.1681, grad_fn=<MseLossBackward0>)\ntensor(21.8044, grad_fn=<MseLossBackward0>)\ntensor(20.7395, grad_fn=<MseLossBackward0>)\ntensor(19.9421, grad_fn=<MseLossBackward0>)\ntensor(20.1680, grad_fn=<MseLossBackward0>)\ntensor(21.7630, grad_fn=<MseLossBackward0>)\ntensor(20.5876, grad_fn=<MseLossBackward0>)\ntensor(18.3887, grad_fn=<MseLossBackward0>)\ntensor(19.9578, grad_fn=<MseLossBackward0>)\ntensor(16.2632, grad_fn=<MseLossBackward0>)\ntensor(20.0788, grad_fn=<MseLossBackward0>)\ntensor(15.6113, grad_fn=<MseLossBackward0>)\ntensor(16.2236, grad_fn=<MseLossBackward0>)\ntensor(18.7069, grad_fn=<MseLossBackward0>)\ntensor(16.1030, grad_fn=<MseLossBackward0>)\ntensor(16.6522, grad_fn=<MseLossBackward0>)\ntensor(15.8259, grad_fn=<MseLossBackward0>)\ntensor(15.1292, grad_fn=<MseLossBackward0>)\ntensor(19.6145, grad_fn=<MseLossBackward0>)\ntensor(18.0620, grad_fn=<MseLossBackward0>)\ntensor(17.1146, grad_fn=<MseLossBackward0>)\ntensor(16.8689, grad_fn=<MseLossBackward0>)\ntensor(19.8749, grad_fn=<MseLossBackward0>)\ntensor(15.5680, grad_fn=<MseLossBackward0>)\ntensor(19.0425, grad_fn=<MseLossBackward0>)\ntensor(16.5391, grad_fn=<MseLossBackward0>)\ntensor(17.6388, grad_fn=<MseLossBackward0>)\ntensor(15.5326, grad_fn=<MseLossBackward0>)\ntensor(19.9664, grad_fn=<MseLossBackward0>)\ntensor(12.4704, grad_fn=<MseLossBackward0>)\ntensor(15.5214, grad_fn=<MseLossBackward0>)\ntensor(18.8696, grad_fn=<MseLossBackward0>)\ntensor(16.3475, grad_fn=<MseLossBackward0>)\ntensor(18.8428, grad_fn=<MseLossBackward0>)\ntensor(13.0338, grad_fn=<MseLossBackward0>)\ntensor(14.5775, grad_fn=<MseLossBackward0>)\ntensor(14.8791, grad_fn=<MseLossBackward0>)\ntensor(17.1218, grad_fn=<MseLossBackward0>)\ntensor(17.4400, grad_fn=<MseLossBackward0>)\ntensor(17.3111, grad_fn=<MseLossBackward0>)\ntensor(22.2436, grad_fn=<MseLossBackward0>)\ntensor(21.2673, grad_fn=<MseLossBackward0>)\ntensor(22.0187, grad_fn=<MseLossBackward0>)\ntensor(18.2290, grad_fn=<MseLossBackward0>)\ntensor(19.0389, grad_fn=<MseLossBackward0>)\ntensor(19.0192, grad_fn=<MseLossBackward0>)\ntensor(15.6372, grad_fn=<MseLossBackward0>)\ntensor(17.3077, grad_fn=<MseLossBackward0>)\ntensor(18.0444, grad_fn=<MseLossBackward0>)\ntensor(17.4057, grad_fn=<MseLossBackward0>)\ntensor(18.9320, grad_fn=<MseLossBackward0>)\ntensor(15.1450, grad_fn=<MseLossBackward0>)\ntensor(14.8156, grad_fn=<MseLossBackward0>)\ntensor(15.8919, grad_fn=<MseLossBackward0>)\ntensor(16.5780, grad_fn=<MseLossBackward0>)\ntensor(18.5005, grad_fn=<MseLossBackward0>)\ntensor(14.6502, grad_fn=<MseLossBackward0>)\ntensor(15.3350, grad_fn=<MseLossBackward0>)\ntensor(14.6596, grad_fn=<MseLossBackward0>)\ntensor(18.5487, grad_fn=<MseLossBackward0>)\ntensor(15.1698, grad_fn=<MseLossBackward0>)\ntensor(20.7597, grad_fn=<MseLossBackward0>)\ntensor(15.1823, grad_fn=<MseLossBackward0>)\ntensor(11.5907, grad_fn=<MseLossBackward0>)\ntensor(13.0682, grad_fn=<MseLossBackward0>)\ntensor(14.9529, grad_fn=<MseLossBackward0>)\ntensor(17.1768, grad_fn=<MseLossBackward0>)\ntensor(10.8170, grad_fn=<MseLossBackward0>)\ntensor(14.2386, grad_fn=<MseLossBackward0>)\ntensor(12.2659, grad_fn=<MseLossBackward0>)\ntensor(8.8636, grad_fn=<MseLossBackward0>)\ntensor(15.3653, grad_fn=<MseLossBackward0>)\ntensor(13.7947, grad_fn=<MseLossBackward0>)\ntensor(13.3946, grad_fn=<MseLossBackward0>)\ntensor(15.4872, grad_fn=<MseLossBackward0>)\ntensor(13.6679, grad_fn=<MseLossBackward0>)\ntensor(14.5508, grad_fn=<MseLossBackward0>)\ntensor(10.7242, grad_fn=<MseLossBackward0>)\ntensor(14.3548, grad_fn=<MseLossBackward0>)\ntensor(13.9399, grad_fn=<MseLossBackward0>)\ntensor(22.1521, grad_fn=<MseLossBackward0>)\ntensor(23.1279, grad_fn=<MseLossBackward0>)\ntensor(22.0842, grad_fn=<MseLossBackward0>)\ntensor(22.1492, grad_fn=<MseLossBackward0>)\ntensor(21.1770, grad_fn=<MseLossBackward0>)\ntensor(23.1136, grad_fn=<MseLossBackward0>)\ntensor(21.7294, grad_fn=<MseLossBackward0>)\ntensor(21.2820, grad_fn=<MseLossBackward0>)\ntensor(19.1111, grad_fn=<MseLossBackward0>)\ntensor(20.0681, grad_fn=<MseLossBackward0>)\ntensor(19.4351, grad_fn=<MseLossBackward0>)\ntensor(19.9499, grad_fn=<MseLossBackward0>)\ntensor(19.7580, grad_fn=<MseLossBackward0>)\ntensor(20.7732, grad_fn=<MseLossBackward0>)\ntensor(17.4257, grad_fn=<MseLossBackward0>)\ntensor(20.8751, grad_fn=<MseLossBackward0>)\ntensor(19.4007, grad_fn=<MseLossBackward0>)\ntensor(20.7421, grad_fn=<MseLossBackward0>)\ntensor(18.7967, grad_fn=<MseLossBackward0>)\ntensor(19.8730, grad_fn=<MseLossBackward0>)\ntensor(20.5888, grad_fn=<MseLossBackward0>)\ntensor(18.7634, grad_fn=<MseLossBackward0>)\ntensor(21.9005, grad_fn=<MseLossBackward0>)\ntensor(18.7982, grad_fn=<MseLossBackward0>)\ntensor(18.3652, grad_fn=<MseLossBackward0>)\ntensor(17.4877, grad_fn=<MseLossBackward0>)\ntensor(19.6463, grad_fn=<MseLossBackward0>)\ntensor(20.6197, grad_fn=<MseLossBackward0>)\ntensor(18.8324, grad_fn=<MseLossBackward0>)\ntensor(19.0565, grad_fn=<MseLossBackward0>)\ntensor(16.7609, grad_fn=<MseLossBackward0>)\ntensor(19.6733, grad_fn=<MseLossBackward0>)\ntensor(19.0381, grad_fn=<MseLossBackward0>)\ntensor(20.7938, grad_fn=<MseLossBackward0>)\ntensor(19.0694, grad_fn=<MseLossBackward0>)\ntensor(19.1028, grad_fn=<MseLossBackward0>)\ntensor(17.1505, grad_fn=<MseLossBackward0>)\ntensor(17.1958, grad_fn=<MseLossBackward0>)\ntensor(19.7691, grad_fn=<MseLossBackward0>)\ntensor(17.4504, grad_fn=<MseLossBackward0>)\ntensor(21.3138, grad_fn=<MseLossBackward0>)\ntensor(21.4674, grad_fn=<MseLossBackward0>)\ntensor(22.5451, grad_fn=<MseLossBackward0>)\ntensor(21.3711, grad_fn=<MseLossBackward0>)\ntensor(19.6491, grad_fn=<MseLossBackward0>)\ntensor(18.5664, grad_fn=<MseLossBackward0>)\ntensor(20.5998, grad_fn=<MseLossBackward0>)\ntensor(18.3481, grad_fn=<MseLossBackward0>)\ntensor(18.1071, grad_fn=<MseLossBackward0>)\ntensor(19.2682, grad_fn=<MseLossBackward0>)\ntensor(18.1346, grad_fn=<MseLossBackward0>)\ntensor(18.8760, grad_fn=<MseLossBackward0>)\ntensor(15.4054, grad_fn=<MseLossBackward0>)\ntensor(18.7032, grad_fn=<MseLossBackward0>)\ntensor(15.7805, grad_fn=<MseLossBackward0>)\ntensor(17.7884, grad_fn=<MseLossBackward0>)\ntensor(16.5666, grad_fn=<MseLossBackward0>)\ntensor(18.2787, grad_fn=<MseLossBackward0>)\ntensor(17.0332, grad_fn=<MseLossBackward0>)\ntensor(17.7182, grad_fn=<MseLossBackward0>)\ntensor(17.3432, grad_fn=<MseLossBackward0>)\ntensor(14.8572, grad_fn=<MseLossBackward0>)\ntensor(13.1194, grad_fn=<MseLossBackward0>)\ntensor(17.9791, grad_fn=<MseLossBackward0>)\ntensor(13.3838, grad_fn=<MseLossBackward0>)\ntensor(17.2637, grad_fn=<MseLossBackward0>)\ntensor(14.5367, grad_fn=<MseLossBackward0>)\ntensor(13.9409, grad_fn=<MseLossBackward0>)\ntensor(15.4272, grad_fn=<MseLossBackward0>)\ntensor(18.2164, grad_fn=<MseLossBackward0>)\ntensor(15.7096, grad_fn=<MseLossBackward0>)\ntensor(13.1458, grad_fn=<MseLossBackward0>)\ntensor(14.7413, grad_fn=<MseLossBackward0>)\ntensor(17.4064, grad_fn=<MseLossBackward0>)\ntensor(20.7482, grad_fn=<MseLossBackward0>)\ntensor(15.4663, grad_fn=<MseLossBackward0>)\ntensor(15.5121, grad_fn=<MseLossBackward0>)\ntensor(15.9684, grad_fn=<MseLossBackward0>)\ntensor(17.8033, grad_fn=<MseLossBackward0>)\ntensor(18.4447, grad_fn=<MseLossBackward0>)\ntensor(23.1965, grad_fn=<MseLossBackward0>)\ntensor(20.6838, grad_fn=<MseLossBackward0>)\ntensor(21.7694, grad_fn=<MseLossBackward0>)\ntensor(19.8973, grad_fn=<MseLossBackward0>)\ntensor(20.8714, grad_fn=<MseLossBackward0>)\ntensor(16.1200, grad_fn=<MseLossBackward0>)\ntensor(19.8564, grad_fn=<MseLossBackward0>)\ntensor(16.9631, grad_fn=<MseLossBackward0>)\ntensor(18.1720, grad_fn=<MseLossBackward0>)\ntensor(17.2165, grad_fn=<MseLossBackward0>)\ntensor(15.3427, grad_fn=<MseLossBackward0>)\ntensor(15.2087, grad_fn=<MseLossBackward0>)\ntensor(17.7404, grad_fn=<MseLossBackward0>)\ntensor(15.3207, grad_fn=<MseLossBackward0>)\ntensor(17.4954, grad_fn=<MseLossBackward0>)\ntensor(15.1712, grad_fn=<MseLossBackward0>)\ntensor(17.0513, grad_fn=<MseLossBackward0>)\ntensor(17.0657, grad_fn=<MseLossBackward0>)\ntensor(16.3999, grad_fn=<MseLossBackward0>)\ntensor(19.0178, grad_fn=<MseLossBackward0>)\ntensor(17.2830, grad_fn=<MseLossBackward0>)\ntensor(16.6661, grad_fn=<MseLossBackward0>)\ntensor(16.4138, grad_fn=<MseLossBackward0>)\ntensor(16.6330, grad_fn=<MseLossBackward0>)\ntensor(15.0451, grad_fn=<MseLossBackward0>)\ntensor(18.3064, grad_fn=<MseLossBackward0>)\ntensor(14.4753, grad_fn=<MseLossBackward0>)\ntensor(15.3327, grad_fn=<MseLossBackward0>)\ntensor(15.7711, grad_fn=<MseLossBackward0>)\ntensor(13.8006, grad_fn=<MseLossBackward0>)\ntensor(17.1306, grad_fn=<MseLossBackward0>)\ntensor(16.1389, grad_fn=<MseLossBackward0>)\ntensor(16.4628, grad_fn=<MseLossBackward0>)\ntensor(17.3231, grad_fn=<MseLossBackward0>)\ntensor(17.2811, grad_fn=<MseLossBackward0>)\ntensor(15.3177, grad_fn=<MseLossBackward0>)\ntensor(14.6196, grad_fn=<MseLossBackward0>)\ntensor(15.2091, grad_fn=<MseLossBackward0>)\ntensor(13.5463, grad_fn=<MseLossBackward0>)\ntensor(17.7424, grad_fn=<MseLossBackward0>)\ntensor(22.2589, grad_fn=<MseLossBackward0>)\ntensor(20.4537, grad_fn=<MseLossBackward0>)\ntensor(18.0500, grad_fn=<MseLossBackward0>)\ntensor(19.8996, grad_fn=<MseLossBackward0>)\ntensor(18.0834, grad_fn=<MseLossBackward0>)\ntensor(16.0057, grad_fn=<MseLossBackward0>)\ntensor(16.5428, grad_fn=<MseLossBackward0>)\ntensor(19.1657, grad_fn=<MseLossBackward0>)\ntensor(13.7850, grad_fn=<MseLossBackward0>)\ntensor(18.9723, grad_fn=<MseLossBackward0>)\ntensor(15.4264, grad_fn=<MseLossBackward0>)\ntensor(14.8101, grad_fn=<MseLossBackward0>)\ntensor(14.4357, grad_fn=<MseLossBackward0>)\ntensor(17.5743, grad_fn=<MseLossBackward0>)\ntensor(15.0069, grad_fn=<MseLossBackward0>)\ntensor(17.5857, grad_fn=<MseLossBackward0>)\ntensor(16.8661, grad_fn=<MseLossBackward0>)\ntensor(12.5906, grad_fn=<MseLossBackward0>)\ntensor(15.4411, grad_fn=<MseLossBackward0>)\ntensor(17.0300, grad_fn=<MseLossBackward0>)\ntensor(15.7819, grad_fn=<MseLossBackward0>)\ntensor(13.3661, grad_fn=<MseLossBackward0>)\ntensor(14.7605, grad_fn=<MseLossBackward0>)\ntensor(15.4114, grad_fn=<MseLossBackward0>)\ntensor(16.6610, grad_fn=<MseLossBackward0>)\ntensor(15.0737, grad_fn=<MseLossBackward0>)\ntensor(12.6674, grad_fn=<MseLossBackward0>)\ntensor(13.7215, grad_fn=<MseLossBackward0>)\ntensor(15.4242, grad_fn=<MseLossBackward0>)\ntensor(13.7145, grad_fn=<MseLossBackward0>)\ntensor(14.9509, grad_fn=<MseLossBackward0>)\ntensor(17.1310, grad_fn=<MseLossBackward0>)\ntensor(15.9399, grad_fn=<MseLossBackward0>)\ntensor(13.9530, grad_fn=<MseLossBackward0>)\ntensor(10.2489, grad_fn=<MseLossBackward0>)\ntensor(14.5855, grad_fn=<MseLossBackward0>)\ntensor(15.6613, grad_fn=<MseLossBackward0>)\ntensor(11.6275, grad_fn=<MseLossBackward0>)\ntensor(14.0439, grad_fn=<MseLossBackward0>)\ntensor(16.6042, grad_fn=<MseLossBackward0>)\ntensor(22.9893, grad_fn=<MseLossBackward0>)\ntensor(21.6932, grad_fn=<MseLossBackward0>)\ntensor(21.3817, grad_fn=<MseLossBackward0>)\ntensor(21.4073, grad_fn=<MseLossBackward0>)\ntensor(19.7515, grad_fn=<MseLossBackward0>)\ntensor(19.5636, grad_fn=<MseLossBackward0>)\ntensor(21.8687, grad_fn=<MseLossBackward0>)\ntensor(21.5894, grad_fn=<MseLossBackward0>)\ntensor(20.1455, grad_fn=<MseLossBackward0>)\ntensor(19.9353, grad_fn=<MseLossBackward0>)\ntensor(20.1187, grad_fn=<MseLossBackward0>)\ntensor(21.3125, grad_fn=<MseLossBackward0>)\ntensor(21.6347, grad_fn=<MseLossBackward0>)\ntensor(21.0649, grad_fn=<MseLossBackward0>)\ntensor(21.9633, grad_fn=<MseLossBackward0>)\ntensor(19.8895, grad_fn=<MseLossBackward0>)\ntensor(20.9581, grad_fn=<MseLossBackward0>)\ntensor(20.4730, grad_fn=<MseLossBackward0>)\ntensor(19.0790, grad_fn=<MseLossBackward0>)\ntensor(19.4418, grad_fn=<MseLossBackward0>)\ntensor(19.0917, grad_fn=<MseLossBackward0>)\ntensor(20.5607, grad_fn=<MseLossBackward0>)\ntensor(21.5297, grad_fn=<MseLossBackward0>)\ntensor(19.7192, grad_fn=<MseLossBackward0>)\ntensor(19.1097, grad_fn=<MseLossBackward0>)\ntensor(18.1313, grad_fn=<MseLossBackward0>)\ntensor(17.4646, grad_fn=<MseLossBackward0>)\ntensor(20.0806, grad_fn=<MseLossBackward0>)\ntensor(18.5683, grad_fn=<MseLossBackward0>)\ntensor(18.7627, grad_fn=<MseLossBackward0>)\ntensor(21.3162, grad_fn=<MseLossBackward0>)\ntensor(17.9328, grad_fn=<MseLossBackward0>)\ntensor(18.9634, grad_fn=<MseLossBackward0>)\ntensor(20.7074, grad_fn=<MseLossBackward0>)\ntensor(20.7929, grad_fn=<MseLossBackward0>)\ntensor(18.0370, grad_fn=<MseLossBackward0>)\ntensor(16.4891, grad_fn=<MseLossBackward0>)\ntensor(18.7077, grad_fn=<MseLossBackward0>)\ntensor(18.5463, grad_fn=<MseLossBackward0>)\ntensor(18.3255, grad_fn=<MseLossBackward0>)\ntensor(22.5243, grad_fn=<MseLossBackward0>)\ntensor(22.4677, grad_fn=<MseLossBackward0>)\ntensor(20.4517, grad_fn=<MseLossBackward0>)\ntensor(19.9164, grad_fn=<MseLossBackward0>)\ntensor(19.7267, grad_fn=<MseLossBackward0>)\ntensor(21.5948, grad_fn=<MseLossBackward0>)\ntensor(21.9289, grad_fn=<MseLossBackward0>)\ntensor(16.2741, grad_fn=<MseLossBackward0>)\ntensor(19.5993, grad_fn=<MseLossBackward0>)\ntensor(19.5473, grad_fn=<MseLossBackward0>)\ntensor(18.2439, grad_fn=<MseLossBackward0>)\ntensor(19.3370, grad_fn=<MseLossBackward0>)\ntensor(16.2643, grad_fn=<MseLossBackward0>)\ntensor(17.1539, grad_fn=<MseLossBackward0>)\ntensor(16.9202, grad_fn=<MseLossBackward0>)\ntensor(17.6552, grad_fn=<MseLossBackward0>)\ntensor(17.0617, grad_fn=<MseLossBackward0>)\ntensor(15.8684, grad_fn=<MseLossBackward0>)\ntensor(15.2721, grad_fn=<MseLossBackward0>)\ntensor(16.7098, grad_fn=<MseLossBackward0>)\ntensor(18.4644, grad_fn=<MseLossBackward0>)\ntensor(17.0701, grad_fn=<MseLossBackward0>)\ntensor(17.7474, grad_fn=<MseLossBackward0>)\ntensor(16.0075, grad_fn=<MseLossBackward0>)\ntensor(16.5503, grad_fn=<MseLossBackward0>)\ntensor(14.5010, grad_fn=<MseLossBackward0>)\ntensor(18.2081, grad_fn=<MseLossBackward0>)\ntensor(17.7715, grad_fn=<MseLossBackward0>)\ntensor(15.8953, grad_fn=<MseLossBackward0>)\ntensor(15.3279, grad_fn=<MseLossBackward0>)\ntensor(13.4986, grad_fn=<MseLossBackward0>)\ntensor(14.2610, grad_fn=<MseLossBackward0>)\ntensor(19.9414, grad_fn=<MseLossBackward0>)\ntensor(11.9728, grad_fn=<MseLossBackward0>)\ntensor(14.2217, grad_fn=<MseLossBackward0>)\ntensor(17.9595, grad_fn=<MseLossBackward0>)\ntensor(15.2998, grad_fn=<MseLossBackward0>)\ntensor(15.8071, grad_fn=<MseLossBackward0>)\ntensor(15.3860, grad_fn=<MseLossBackward0>)\ntensor(15.5700, grad_fn=<MseLossBackward0>)\ntensor(22.0808, grad_fn=<MseLossBackward0>)\ntensor(21.1304, grad_fn=<MseLossBackward0>)\ntensor(20.4850, grad_fn=<MseLossBackward0>)\ntensor(19.5872, grad_fn=<MseLossBackward0>)\ntensor(18.0872, grad_fn=<MseLossBackward0>)\ntensor(18.9966, grad_fn=<MseLossBackward0>)\ntensor(19.3582, grad_fn=<MseLossBackward0>)\ntensor(19.6264, grad_fn=<MseLossBackward0>)\ntensor(20.3633, grad_fn=<MseLossBackward0>)\ntensor(19.3230, grad_fn=<MseLossBackward0>)\ntensor(17.4650, grad_fn=<MseLossBackward0>)\ntensor(15.3954, grad_fn=<MseLossBackward0>)\ntensor(16.6567, grad_fn=<MseLossBackward0>)\ntensor(20.8141, grad_fn=<MseLossBackward0>)\ntensor(16.2212, grad_fn=<MseLossBackward0>)\ntensor(17.9977, grad_fn=<MseLossBackward0>)\ntensor(17.6131, grad_fn=<MseLossBackward0>)\ntensor(15.1941, grad_fn=<MseLossBackward0>)\ntensor(19.6824, grad_fn=<MseLossBackward0>)\ntensor(14.6986, grad_fn=<MseLossBackward0>)\ntensor(18.2565, grad_fn=<MseLossBackward0>)\ntensor(18.4217, grad_fn=<MseLossBackward0>)\ntensor(15.2076, grad_fn=<MseLossBackward0>)\ntensor(17.6681, grad_fn=<MseLossBackward0>)\ntensor(16.9175, grad_fn=<MseLossBackward0>)\ntensor(17.4764, grad_fn=<MseLossBackward0>)\ntensor(15.2923, grad_fn=<MseLossBackward0>)\ntensor(18.2704, grad_fn=<MseLossBackward0>)\ntensor(14.3121, grad_fn=<MseLossBackward0>)\ntensor(16.3996, grad_fn=<MseLossBackward0>)\ntensor(14.3466, grad_fn=<MseLossBackward0>)\ntensor(16.2134, grad_fn=<MseLossBackward0>)\ntensor(18.3816, grad_fn=<MseLossBackward0>)\ntensor(16.8216, grad_fn=<MseLossBackward0>)\ntensor(18.3771, grad_fn=<MseLossBackward0>)\ntensor(12.6809, grad_fn=<MseLossBackward0>)\ntensor(14.4929, grad_fn=<MseLossBackward0>)\ntensor(12.4764, grad_fn=<MseLossBackward0>)\ntensor(17.6538, grad_fn=<MseLossBackward0>)\ntensor(13.4455, grad_fn=<MseLossBackward0>)\ntensor(22.0660, grad_fn=<MseLossBackward0>)\ntensor(20.3933, grad_fn=<MseLossBackward0>)\ntensor(18.3330, grad_fn=<MseLossBackward0>)\ntensor(18.0241, grad_fn=<MseLossBackward0>)\ntensor(17.3249, grad_fn=<MseLossBackward0>)\ntensor(17.5484, grad_fn=<MseLossBackward0>)\ntensor(16.0338, grad_fn=<MseLossBackward0>)\ntensor(16.2001, grad_fn=<MseLossBackward0>)\ntensor(18.0676, grad_fn=<MseLossBackward0>)\ntensor(12.3317, grad_fn=<MseLossBackward0>)\ntensor(15.9135, grad_fn=<MseLossBackward0>)\ntensor(15.3875, grad_fn=<MseLossBackward0>)\ntensor(16.5657, grad_fn=<MseLossBackward0>)\ntensor(15.7004, grad_fn=<MseLossBackward0>)\ntensor(16.2766, grad_fn=<MseLossBackward0>)\ntensor(12.2174, grad_fn=<MseLossBackward0>)\ntensor(12.6446, grad_fn=<MseLossBackward0>)\ntensor(10.0434, grad_fn=<MseLossBackward0>)\ntensor(14.5245, grad_fn=<MseLossBackward0>)\ntensor(14.3966, grad_fn=<MseLossBackward0>)\ntensor(14.0999, grad_fn=<MseLossBackward0>)\ntensor(11.9425, grad_fn=<MseLossBackward0>)\ntensor(11.7782, grad_fn=<MseLossBackward0>)\ntensor(14.6019, grad_fn=<MseLossBackward0>)\ntensor(11.0396, grad_fn=<MseLossBackward0>)\ntensor(14.8573, grad_fn=<MseLossBackward0>)\ntensor(10.4585, grad_fn=<MseLossBackward0>)\ntensor(8.0628, grad_fn=<MseLossBackward0>)\ntensor(14.2667, grad_fn=<MseLossBackward0>)\ntensor(11.1330, grad_fn=<MseLossBackward0>)\ntensor(15.4090, grad_fn=<MseLossBackward0>)\ntensor(14.4436, grad_fn=<MseLossBackward0>)\ntensor(12.5146, grad_fn=<MseLossBackward0>)\ntensor(8.9819, grad_fn=<MseLossBackward0>)\ntensor(14.7374, grad_fn=<MseLossBackward0>)\ntensor(11.2598, grad_fn=<MseLossBackward0>)\ntensor(9.3100, grad_fn=<MseLossBackward0>)\ntensor(10.7679, grad_fn=<MseLossBackward0>)\ntensor(8.8727, grad_fn=<MseLossBackward0>)\ntensor(13.9257, grad_fn=<MseLossBackward0>)\ntensor(22.6029, grad_fn=<MseLossBackward0>)\ntensor(22.3269, grad_fn=<MseLossBackward0>)\ntensor(21.4478, grad_fn=<MseLossBackward0>)\ntensor(22.1455, grad_fn=<MseLossBackward0>)\ntensor(19.9633, grad_fn=<MseLossBackward0>)\ntensor(21.7321, grad_fn=<MseLossBackward0>)\ntensor(19.6844, grad_fn=<MseLossBackward0>)\ntensor(21.2990, grad_fn=<MseLossBackward0>)\ntensor(20.5797, grad_fn=<MseLossBackward0>)\ntensor(20.5359, grad_fn=<MseLossBackward0>)\ntensor(20.2444, grad_fn=<MseLossBackward0>)\ntensor(21.2401, grad_fn=<MseLossBackward0>)\ntensor(19.4228, grad_fn=<MseLossBackward0>)\ntensor(17.5111, grad_fn=<MseLossBackward0>)\ntensor(20.0040, grad_fn=<MseLossBackward0>)\ntensor(18.6856, grad_fn=<MseLossBackward0>)\ntensor(17.6104, grad_fn=<MseLossBackward0>)\ntensor(19.6659, grad_fn=<MseLossBackward0>)\ntensor(19.6947, grad_fn=<MseLossBackward0>)\ntensor(20.6344, grad_fn=<MseLossBackward0>)\ntensor(20.4360, grad_fn=<MseLossBackward0>)\ntensor(16.9952, grad_fn=<MseLossBackward0>)\ntensor(19.2720, grad_fn=<MseLossBackward0>)\ntensor(20.4195, grad_fn=<MseLossBackward0>)\ntensor(19.5670, grad_fn=<MseLossBackward0>)\ntensor(18.9826, grad_fn=<MseLossBackward0>)\ntensor(19.1652, grad_fn=<MseLossBackward0>)\ntensor(17.9461, grad_fn=<MseLossBackward0>)\ntensor(20.5292, grad_fn=<MseLossBackward0>)\ntensor(17.8245, grad_fn=<MseLossBackward0>)\ntensor(19.1938, grad_fn=<MseLossBackward0>)\ntensor(20.2587, grad_fn=<MseLossBackward0>)\ntensor(20.4349, grad_fn=<MseLossBackward0>)\ntensor(19.0124, grad_fn=<MseLossBackward0>)\ntensor(18.3937, grad_fn=<MseLossBackward0>)\ntensor(17.4692, grad_fn=<MseLossBackward0>)\ntensor(19.4886, grad_fn=<MseLossBackward0>)\ntensor(18.2312, grad_fn=<MseLossBackward0>)\ntensor(20.2122, grad_fn=<MseLossBackward0>)\ntensor(18.5954, grad_fn=<MseLossBackward0>)\ntensor(22.5075, grad_fn=<MseLossBackward0>)\ntensor(22.7922, grad_fn=<MseLossBackward0>)\ntensor(20.8723, grad_fn=<MseLossBackward0>)\ntensor(19.3176, grad_fn=<MseLossBackward0>)\ntensor(20.8812, grad_fn=<MseLossBackward0>)\ntensor(19.0605, grad_fn=<MseLossBackward0>)\ntensor(16.7482, grad_fn=<MseLossBackward0>)\ntensor(17.7143, grad_fn=<MseLossBackward0>)\ntensor(15.9932, grad_fn=<MseLossBackward0>)\ntensor(18.9768, grad_fn=<MseLossBackward0>)\ntensor(17.7700, grad_fn=<MseLossBackward0>)\ntensor(15.5539, grad_fn=<MseLossBackward0>)\ntensor(18.4999, grad_fn=<MseLossBackward0>)\ntensor(12.5063, grad_fn=<MseLossBackward0>)\ntensor(15.8980, grad_fn=<MseLossBackward0>)\ntensor(16.6000, grad_fn=<MseLossBackward0>)\ntensor(17.0440, grad_fn=<MseLossBackward0>)\ntensor(16.6318, grad_fn=<MseLossBackward0>)\ntensor(16.2532, grad_fn=<MseLossBackward0>)\ntensor(16.9217, grad_fn=<MseLossBackward0>)\ntensor(17.6305, grad_fn=<MseLossBackward0>)\ntensor(15.5798, grad_fn=<MseLossBackward0>)\ntensor(13.0339, grad_fn=<MseLossBackward0>)\ntensor(18.6791, grad_fn=<MseLossBackward0>)\ntensor(17.7328, grad_fn=<MseLossBackward0>)\ntensor(15.3091, grad_fn=<MseLossBackward0>)\ntensor(17.3429, grad_fn=<MseLossBackward0>)\ntensor(16.6070, grad_fn=<MseLossBackward0>)\ntensor(16.4930, grad_fn=<MseLossBackward0>)\ntensor(14.8903, grad_fn=<MseLossBackward0>)\ntensor(14.5620, grad_fn=<MseLossBackward0>)\ntensor(14.1758, grad_fn=<MseLossBackward0>)\ntensor(14.1047, grad_fn=<MseLossBackward0>)\ntensor(14.0575, grad_fn=<MseLossBackward0>)\ntensor(15.1031, grad_fn=<MseLossBackward0>)\ntensor(13.6928, grad_fn=<MseLossBackward0>)\ntensor(13.4216, grad_fn=<MseLossBackward0>)\ntensor(18.6361, grad_fn=<MseLossBackward0>)\ntensor(15.8736, grad_fn=<MseLossBackward0>)\ntensor(17.4098, grad_fn=<MseLossBackward0>)\ntensor(21.4310, grad_fn=<MseLossBackward0>)\ntensor(19.1135, grad_fn=<MseLossBackward0>)\ntensor(19.9265, grad_fn=<MseLossBackward0>)\ntensor(16.9430, grad_fn=<MseLossBackward0>)\ntensor(17.4785, grad_fn=<MseLossBackward0>)\ntensor(19.0467, grad_fn=<MseLossBackward0>)\ntensor(18.7892, grad_fn=<MseLossBackward0>)\ntensor(17.9180, grad_fn=<MseLossBackward0>)\ntensor(18.8749, grad_fn=<MseLossBackward0>)\ntensor(18.1557, grad_fn=<MseLossBackward0>)\ntensor(16.7365, grad_fn=<MseLossBackward0>)\ntensor(15.9218, grad_fn=<MseLossBackward0>)\ntensor(18.4920, grad_fn=<MseLossBackward0>)\ntensor(18.9642, grad_fn=<MseLossBackward0>)\ntensor(15.3756, grad_fn=<MseLossBackward0>)\ntensor(15.9748, grad_fn=<MseLossBackward0>)\ntensor(15.6060, grad_fn=<MseLossBackward0>)\ntensor(16.2146, grad_fn=<MseLossBackward0>)\ntensor(15.2157, grad_fn=<MseLossBackward0>)\ntensor(15.3188, grad_fn=<MseLossBackward0>)\ntensor(14.5561, grad_fn=<MseLossBackward0>)\ntensor(18.1402, grad_fn=<MseLossBackward0>)\ntensor(13.2894, grad_fn=<MseLossBackward0>)\ntensor(14.6892, grad_fn=<MseLossBackward0>)\ntensor(14.1388, grad_fn=<MseLossBackward0>)\ntensor(19.4755, grad_fn=<MseLossBackward0>)\ntensor(13.0042, grad_fn=<MseLossBackward0>)\ntensor(15.6029, grad_fn=<MseLossBackward0>)\ntensor(16.5028, grad_fn=<MseLossBackward0>)\ntensor(15.7006, grad_fn=<MseLossBackward0>)\ntensor(18.6744, grad_fn=<MseLossBackward0>)\ntensor(13.8016, grad_fn=<MseLossBackward0>)\ntensor(17.0063, grad_fn=<MseLossBackward0>)\ntensor(14.2345, grad_fn=<MseLossBackward0>)\ntensor(15.3635, grad_fn=<MseLossBackward0>)\ntensor(13.5488, grad_fn=<MseLossBackward0>)\ntensor(13.7194, grad_fn=<MseLossBackward0>)\ntensor(13.1732, grad_fn=<MseLossBackward0>)\ntensor(15.7651, grad_fn=<MseLossBackward0>)\ntensor(14.3185, grad_fn=<MseLossBackward0>)\ntensor(22.6765, grad_fn=<MseLossBackward0>)\ntensor(19.2892, grad_fn=<MseLossBackward0>)\ntensor(17.2936, grad_fn=<MseLossBackward0>)\ntensor(20.3195, grad_fn=<MseLossBackward0>)\ntensor(19.8449, grad_fn=<MseLossBackward0>)\ntensor(18.4632, grad_fn=<MseLossBackward0>)\ntensor(13.7603, grad_fn=<MseLossBackward0>)\ntensor(15.3433, grad_fn=<MseLossBackward0>)\ntensor(17.0611, grad_fn=<MseLossBackward0>)\ntensor(18.6023, grad_fn=<MseLossBackward0>)\ntensor(16.0928, grad_fn=<MseLossBackward0>)\ntensor(16.7670, grad_fn=<MseLossBackward0>)\ntensor(16.8607, grad_fn=<MseLossBackward0>)\ntensor(17.1997, grad_fn=<MseLossBackward0>)\ntensor(14.7511, grad_fn=<MseLossBackward0>)\ntensor(13.8208, grad_fn=<MseLossBackward0>)\ntensor(15.8270, grad_fn=<MseLossBackward0>)\ntensor(14.5072, grad_fn=<MseLossBackward0>)\ntensor(16.4845, grad_fn=<MseLossBackward0>)\ntensor(14.1771, grad_fn=<MseLossBackward0>)\ntensor(14.4459, grad_fn=<MseLossBackward0>)\ntensor(16.8399, grad_fn=<MseLossBackward0>)\ntensor(16.1843, grad_fn=<MseLossBackward0>)\ntensor(13.4695, grad_fn=<MseLossBackward0>)\ntensor(13.0707, grad_fn=<MseLossBackward0>)\ntensor(13.3302, grad_fn=<MseLossBackward0>)\ntensor(9.8031, grad_fn=<MseLossBackward0>)\ntensor(14.6313, grad_fn=<MseLossBackward0>)\ntensor(13.7265, grad_fn=<MseLossBackward0>)\ntensor(11.0913, grad_fn=<MseLossBackward0>)\ntensor(11.6456, grad_fn=<MseLossBackward0>)\ntensor(11.9586, grad_fn=<MseLossBackward0>)\ntensor(17.8558, grad_fn=<MseLossBackward0>)\ntensor(15.1599, grad_fn=<MseLossBackward0>)\ntensor(16.2172, grad_fn=<MseLossBackward0>)\ntensor(13.6137, grad_fn=<MseLossBackward0>)\ntensor(12.7476, grad_fn=<MseLossBackward0>)\ntensor(10.6940, grad_fn=<MseLossBackward0>)\ntensor(13.4315, grad_fn=<MseLossBackward0>)\ntensor(11.7746, grad_fn=<MseLossBackward0>)\ntensor(22.9318, grad_fn=<MseLossBackward0>)\ntensor(21.5790, grad_fn=<MseLossBackward0>)\ntensor(22.4430, grad_fn=<MseLossBackward0>)\ntensor(22.1387, grad_fn=<MseLossBackward0>)\ntensor(21.3819, grad_fn=<MseLossBackward0>)\ntensor(21.6115, grad_fn=<MseLossBackward0>)\ntensor(20.0289, grad_fn=<MseLossBackward0>)\ntensor(21.3859, grad_fn=<MseLossBackward0>)\ntensor(21.8153, grad_fn=<MseLossBackward0>)\ntensor(20.7345, grad_fn=<MseLossBackward0>)\ntensor(20.4809, grad_fn=<MseLossBackward0>)\ntensor(19.5879, grad_fn=<MseLossBackward0>)\ntensor(21.3761, grad_fn=<MseLossBackward0>)\ntensor(21.4453, grad_fn=<MseLossBackward0>)\ntensor(19.1533, grad_fn=<MseLossBackward0>)\ntensor(19.8051, grad_fn=<MseLossBackward0>)\ntensor(20.0944, grad_fn=<MseLossBackward0>)\ntensor(19.8963, grad_fn=<MseLossBackward0>)\ntensor(21.0601, grad_fn=<MseLossBackward0>)\ntensor(20.0834, grad_fn=<MseLossBackward0>)\ntensor(19.3518, grad_fn=<MseLossBackward0>)\ntensor(19.3756, grad_fn=<MseLossBackward0>)\ntensor(19.3493, grad_fn=<MseLossBackward0>)\ntensor(20.6684, grad_fn=<MseLossBackward0>)\ntensor(20.3769, grad_fn=<MseLossBackward0>)\ntensor(22.0400, grad_fn=<MseLossBackward0>)\ntensor(19.3818, grad_fn=<MseLossBackward0>)\ntensor(19.0485, grad_fn=<MseLossBackward0>)\ntensor(17.6573, grad_fn=<MseLossBackward0>)\ntensor(20.1187, grad_fn=<MseLossBackward0>)\ntensor(18.1441, grad_fn=<MseLossBackward0>)\ntensor(20.3147, grad_fn=<MseLossBackward0>)\ntensor(18.8746, grad_fn=<MseLossBackward0>)\ntensor(18.3767, grad_fn=<MseLossBackward0>)\ntensor(19.1905, grad_fn=<MseLossBackward0>)\ntensor(18.7521, grad_fn=<MseLossBackward0>)\ntensor(20.3643, grad_fn=<MseLossBackward0>)\ntensor(21.1183, grad_fn=<MseLossBackward0>)\ntensor(19.7058, grad_fn=<MseLossBackward0>)\ntensor(17.9970, grad_fn=<MseLossBackward0>)\ntensor(22.4827, grad_fn=<MseLossBackward0>)\ntensor(22.5729, grad_fn=<MseLossBackward0>)\ntensor(22.7216, grad_fn=<MseLossBackward0>)\ntensor(22.2053, grad_fn=<MseLossBackward0>)\ntensor(20.4890, grad_fn=<MseLossBackward0>)\ntensor(20.8428, grad_fn=<MseLossBackward0>)\ntensor(19.6898, grad_fn=<MseLossBackward0>)\ntensor(18.8326, grad_fn=<MseLossBackward0>)\ntensor(20.1265, grad_fn=<MseLossBackward0>)\ntensor(18.0692, grad_fn=<MseLossBackward0>)\ntensor(19.4655, grad_fn=<MseLossBackward0>)\ntensor(18.4296, grad_fn=<MseLossBackward0>)\ntensor(17.6359, grad_fn=<MseLossBackward0>)\ntensor(17.5209, grad_fn=<MseLossBackward0>)\ntensor(16.3108, grad_fn=<MseLossBackward0>)\ntensor(19.0023, grad_fn=<MseLossBackward0>)\ntensor(15.1780, grad_fn=<MseLossBackward0>)\ntensor(16.6804, grad_fn=<MseLossBackward0>)\ntensor(16.6250, grad_fn=<MseLossBackward0>)\ntensor(14.7884, grad_fn=<MseLossBackward0>)\ntensor(15.3670, grad_fn=<MseLossBackward0>)\ntensor(19.9768, grad_fn=<MseLossBackward0>)\ntensor(21.8213, grad_fn=<MseLossBackward0>)\ntensor(18.8256, grad_fn=<MseLossBackward0>)\ntensor(15.8912, grad_fn=<MseLossBackward0>)\ntensor(15.0588, grad_fn=<MseLossBackward0>)\ntensor(13.5433, grad_fn=<MseLossBackward0>)\ntensor(12.6576, grad_fn=<MseLossBackward0>)\ntensor(15.6909, grad_fn=<MseLossBackward0>)\ntensor(15.8657, grad_fn=<MseLossBackward0>)\ntensor(16.6069, grad_fn=<MseLossBackward0>)\ntensor(13.2374, grad_fn=<MseLossBackward0>)\ntensor(14.7047, grad_fn=<MseLossBackward0>)\ntensor(18.3764, grad_fn=<MseLossBackward0>)\ntensor(15.3996, grad_fn=<MseLossBackward0>)\ntensor(16.6625, grad_fn=<MseLossBackward0>)\ntensor(14.9850, grad_fn=<MseLossBackward0>)\ntensor(14.0735, grad_fn=<MseLossBackward0>)\ntensor(14.5597, grad_fn=<MseLossBackward0>)\ntensor(14.4632, grad_fn=<MseLossBackward0>)\ntensor(22.8113, grad_fn=<MseLossBackward0>)\ntensor(21.4986, grad_fn=<MseLossBackward0>)\ntensor(21.4650, grad_fn=<MseLossBackward0>)\ntensor(21.1173, grad_fn=<MseLossBackward0>)\ntensor(19.4008, grad_fn=<MseLossBackward0>)\ntensor(17.4296, grad_fn=<MseLossBackward0>)\ntensor(19.0470, grad_fn=<MseLossBackward0>)\ntensor(19.7745, grad_fn=<MseLossBackward0>)\ntensor(18.2230, grad_fn=<MseLossBackward0>)\ntensor(18.2874, grad_fn=<MseLossBackward0>)\ntensor(17.6975, grad_fn=<MseLossBackward0>)\ntensor(16.7580, grad_fn=<MseLossBackward0>)\ntensor(16.1569, grad_fn=<MseLossBackward0>)\ntensor(16.5785, grad_fn=<MseLossBackward0>)\ntensor(18.7293, grad_fn=<MseLossBackward0>)\ntensor(18.3778, grad_fn=<MseLossBackward0>)\ntensor(17.2710, grad_fn=<MseLossBackward0>)\ntensor(16.7431, grad_fn=<MseLossBackward0>)\ntensor(12.0098, grad_fn=<MseLossBackward0>)\ntensor(16.0783, grad_fn=<MseLossBackward0>)\ntensor(17.6712, grad_fn=<MseLossBackward0>)\ntensor(14.7522, grad_fn=<MseLossBackward0>)\ntensor(18.4142, grad_fn=<MseLossBackward0>)\ntensor(15.1089, grad_fn=<MseLossBackward0>)\ntensor(19.5465, grad_fn=<MseLossBackward0>)\ntensor(18.0843, grad_fn=<MseLossBackward0>)\ntensor(17.6676, grad_fn=<MseLossBackward0>)\ntensor(18.7419, grad_fn=<MseLossBackward0>)\ntensor(13.7815, grad_fn=<MseLossBackward0>)\ntensor(17.0658, grad_fn=<MseLossBackward0>)\ntensor(18.1723, grad_fn=<MseLossBackward0>)\ntensor(16.6116, grad_fn=<MseLossBackward0>)\ntensor(16.0598, grad_fn=<MseLossBackward0>)\ntensor(16.9319, grad_fn=<MseLossBackward0>)\ntensor(17.6011, grad_fn=<MseLossBackward0>)\ntensor(16.1368, grad_fn=<MseLossBackward0>)\ntensor(10.7779, grad_fn=<MseLossBackward0>)\ntensor(16.6268, grad_fn=<MseLossBackward0>)\ntensor(19.8449, grad_fn=<MseLossBackward0>)\ntensor(16.4383, grad_fn=<MseLossBackward0>)\n0.465625\n0.258125\n0.454375\n0.37125\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "58196210-fcb7-4ecf-afc8-f3dabc77fcc6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f833b328",
    "execution_start": 1642161039553,
    "execution_millis": 457,
    "deepnote_cell_type": "code"
   },
   "source": "\n\ndef writing_to_excel():\n    df1 = pd.DataFrame(rightWrongXYZ)\n    df2 = pd.DataFrame(rightWrongXY)\n    df3 = pd.DataFrame(rightWrongXZ)\n    df4 = pd.DataFrame(rightWrongYZ)\n    filepath = 'rightWrongXYZ.xlsx'\n    df1.to_excel(filepath, index=False)\n    filepath = 'rightWrongXY.xlsx'\n    df2.to_excel(filepath, index=False)\n    filepath = 'rightWrongXZ.xlsx'\n    df3.to_excel(filepath, index=False)\n    filepath = 'rightWrongYZ.xlsx'\n    df4.to_excel(filepath, index=False)\n    \nwriting_to_excel()\n## save to xlsx file\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-17709368-953c-4beb-91db-8a8151ff55b6",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d9da1cb8",
    "execution_start": 1642091387837,
    "execution_millis": 408,
    "deepnote_output_heights": [
     328.390625,
     597.1875
    ],
    "output_cleared": true,
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "df = pd.read_csv('armdatadf.csv')\n\nY = df[[\"setup\", \"person\"]]\nX = df[[\"x\",\"y\",\"z\", \"person\"]]\n\nGen_error = 0\nfor i in range(1,11):\n    X_train = X[(X.person != i)].copy()\n    X_train = X_train.drop(\"person\", axis=1)\n    X_train['x'] = (X_train['x']-X_train['x'].mean())/X_train['x'].std()\n    X_train['y'] = (X_train['y']-X_train['y'].mean())/X_train['y'].std()\n    X_train['z'] = (X_train['z']-X_train['z'].mean())/X_train['z'].std()\n\n    X_test  = X[(X.person == i)].copy()\n    X_test = X_test.drop(\"person\", axis=1)\n    X_test['x'] = (X_test['x']-X_train['x'].mean())/X_train['x'].std()\n    X_test['y'] = (X_test['y']-X_train['y'].mean())/X_train['y'].std()\n    X_test['z'] = (X_test['z']-X_train['z'].mean())/X_train['z'].std()\n\n    #print(X_test.shape)\n    y_train = Y[(Y.person != i)].copy()\n    y_train = y_train.drop(\"person\", axis=1)\n    y_train = y_train-1\n    \n    y_test  = Y[(Y.person == i)].copy()\n    y_test = y_test.drop(\"person\", axis=1)\n    y_test = y_test-1\n    #Training site\n    model = ANNTrain(X_train,y_train)\n\n    #Testing site\n    Gen_error += ANNTest(X_test, y_test, model)\n    \n    #the returned generalization error when testing   (number of correctly classified setups in test)/(length of trajectories in test)\n\nprint(Gen_error/(16*10*10))\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-05a3e2cb-7efb-48a1-b88e-f3baa756c038",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "e7b830f7",
    "execution_start": 1642074151193,
    "execution_millis": 0,
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "def ANNTrain(X_train, y_train, h=25, Batch_size=25, number_of_training_iterations_ANN=1500):    \n\n    #y_train = (y_train-np.mean(y_train))/np.std(y_train)\n    X_train = X_train.to_numpy()\n    y_train = y_train.to_numpy()\n\n    #hyperparameters\n    n_inputs = 300\n    n_outputs = 16\n\n    #Single hidden layer neural network with h hidden neurons\n    model = torch.nn.Sequential(\n    torch.nn.Linear(n_inputs, h),\n    torch.nn.Tanh(),\n    torch.nn.Linear(h, n_outputs),\n    torch.nn.Tanh(),\n    torch.nn.Linear(h, n_outputs),\n    torch.nn.Softmax()\n    )\n\n    #defining loss type\n    loss_fn = torch.nn.MSELoss(reduction='sum') \n    X_train = torch.from_numpy(X_train).to(torch.float32)\n    y_train = torch.from_numpy(y_train).to(torch.float32)\n    learning_rate = 0.005\n\n\n    for t in range(number_of_training_iterations_ANN):\n        indexes = np.random.randint(9*16*10, size=Batch_size) #for every person in the X_train dataset, any setup and any repetition. \n        x = torch.zeros([Batch_size,300]) #25 dataelements, follows x1...x100, så y1..y100 så z1...z100\n        for i in range(Batch_size):\n            x[i,0:100]   = X_train[indexes[i]*100:indexes[i]*100+100,0].squeeze()\n            x[i,100:200] = X_train[indexes[i]*100:indexes[i]*100+100,1].squeeze()\n            x[i,200:300] = X_train[indexes[i]*100:indexes[i]*100+100,2].squeeze()\n\n\n        y = torch.zeros([Batch_size,1])\n        for i in range(Batch_size):\n            y[i] = y_train[indexes[i]*100]\n        y = y.to(torch.int64).t().squeeze()\n        y = F.one_hot(y, num_classes=16).to(torch.float32)\n        \n        y_pred = model(x)\n        print(y.shape)\n        print(y_pred.shape)\n        loss = loss_fn(y_pred, y)\n        \n        #delete the following after model is tested\n        if t % 100 == 99:\n            print(loss)\n\n        model.zero_grad()\n        #computing gradients\n        loss.backward()\n\n        #update parameters with the computed gradients\n        with torch.no_grad():\n            for param in model.parameters():\n                param -= learning_rate * param.grad\n    return model\n\ndef ANNTest(X_test, y_test, model):\n    X_test = X_test.to_numpy()\n    y_test = y_test.to_numpy()\n    y_test = torch.from_numpy(y_test).to(torch.float32) #only X_test need be converted to a torch tensor\n    errors = 0\n    y = torch.zeros([10*16,1])\n    for i in range(10*16):\n        y[i] = y_test[i*100]\n\n    y = y.to(torch.int64).t().squeeze()\n    y = F.one_hot(y, num_classes=16).to(torch.float32)\n    X_test = torch.from_numpy(X_test).to(torch.float32)\n    \n    x = torch.zeros([16*10,300]) #25 dataelements, follows x1...x100, så y1..y100 så z1...z100\n    indexes = np.arange(16*10)\n    for i in range(16*10):\n        x[i,0:100]   = X_test[indexes[i]*100:indexes[i]*100+100,0].squeeze()\n        x[i,100:200] = X_test[indexes[i]*100:indexes[i]*100+100,1].squeeze()\n        x[i,200:300] = X_test[indexes[i]*100:indexes[i]*100+100,2].squeeze()\n    errors = 0.0\n    for i in range(16*10):\n        with torch.no_grad():\n            inp = x[i,0:300]\n            output = model(inp)\n\n            predIndex = torch.argmax(output)\n            if (predIndex > 15):\n                predIndex = 15\n                print(\"nope\")\n            prediction = torch.zeros([16])\n            prediction[predIndex] = 1\n\n            middleStep = y[i]-prediction\n\n\n            errors += torch.max(middleStep).item()\n\n    return errors\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Statistics section",
   "metadata": {
    "cell_id": "e1ae9e90-520f-4877-a1de-ea61be9b4507",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c766b815-b494-49a9-ae16-600a49436cd4",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a200f0b0",
    "execution_start": 1642152752911,
    "execution_millis": 10109,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install statsmodels==0.13.1",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting statsmodels==0.13.1\n  Downloading statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n\u001b[K     |████████████████████████████████| 9.8 MB 19.9 MB/s \n\u001b[?25hRequirement already satisfied: scipy>=1.3 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from statsmodels==0.13.1) (1.7.3)\nRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from statsmodels==0.13.1) (1.19.5)\nRequirement already satisfied: pandas>=0.25 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from statsmodels==0.13.1) (1.2.5)\nCollecting patsy>=0.5.2\n  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n\u001b[K     |████████████████████████████████| 233 kB 39.7 MB/s \n\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pandas>=0.25->statsmodels==0.13.1) (2021.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pandas>=0.25->statsmodels==0.13.1) (2.8.2)\nRequirement already satisfied: six in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels==0.13.1) (1.16.0)\nInstalling collected packages: patsy, statsmodels\nSuccessfully installed patsy-0.5.2 statsmodels-0.13.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a8c9ef0f-e130-4fc9-a7d2-0a685fd494c8",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "def mcnemar(y_true, yhatA, yhatB, alpha=0.05):\n    # perform McNemars test\n    nn = np.zeros((2,2))\n    c1 = yhatA - y_true == 0\n    c2 = yhatB - y_true == 0\n\n    nn[0,0] = sum(c1 & c2)\n    nn[0,1] = sum(c1 & ~c2)\n    nn[1,0] = sum(~c1 & c2)\n    nn[1,1] = sum(~c1 & ~c2)\n\n    n = sum(nn.flat);\n    n12 = nn[0,1]\n    n21 = nn[1,0]\n\n    thetahat = (n12-n21)/n\n    Etheta = thetahat\n\n    Q = n**2 * (n+1) * (Etheta+1) * (1-Etheta) / ( (n*(n12+n21) - (n12-n21)**2) )\n\n    p = (Etheta + 1)*0.5 * (Q-1)\n    q = (1-Etheta)*0.5 * (Q-1)\n\n    \n\n    p = 2*scipy.stats.binom.cdf(min([n12,n21]), n=n12+n21, p=0.5)\n    print(\"Result of McNemars test using alpha=\", alpha)\n    print(\"Comparison matrix n\")\n    print(nn)\n    if n12+n21 <= 10:\n        print(\"Warning, n12+n21 is low: n12+n21=\",(n12+n21))\n\n    print(\"Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = \", CI)\n    print(\"p-value for two-sided test A and B have same accuracy (exact binomial test): p=\", p)\n\n    return thetahat, CI, p",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### AUGUST AFSNIT:Decision trees and KNN",
   "metadata": {
    "cell_id": "cd53664c-d020-4a05-8576-473d6154305c",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "15a0464e-a559-4ee3-9500-0f0840ff8944",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8632adf6",
    "execution_start": 1642113715480,
    "execution_millis": 4219630,
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\n\nimport numpy as np\nfrom collections import Counter\n\nimport pandas as pd\n\n\n\n\n\n\ndef most_frequent(List):\n    return max(set(List), key = List.count)\n\ndef knn_model_train(X_train, y_train, neighbors): \n    knn_models = [None]*100\n    for i in range(100):\n        clf = KNeighborsClassifier(n_neighbors=neighbors)\n        local_X_train = X_train[X_train.time==i+1].copy().drop(columns = [\"time\"])\n        local_Y_train = Y_train[Y_train.time==i+1].setup\n        knn_models[i] = clf.fit(local_X_train, local_Y_train.values.ravel())\n    return knn_models\n\ndef DT_model_train(X_train, y_train): \n    DT_models = [None]*100\n    \n    for i in range(100):\n        clf = tree.DecisionTreeClassifier()\n        local_X_train = X_train[X_train.time==i+1].copy().drop(columns = [\"time\"])\n        local_Y_train = Y_train[Y_train.time==i+1].setup\n        DT_models[i] = clf.fit(local_X_train, local_Y_train.values.ravel())\n    return DT_models\n\ndef model_test(X_test, y_test, models):\n    prediction = []\n    for j in range(160):\n        prediction_vote = []\n        for i in range(100):\n            m = i\n            i += j*100\n            y_hat = models[m].predict(X_test[i:i+1]).item(0)\n            prediction_vote.append(y_hat)\n        prediction.append(most_frequent(prediction_vote))\n    facit = y_test[\"setup\"][::100]\n    return facit, prediction    \n\n\n\n\n\n\ndf = pd.read_csv('armdatadf.csv')\n#df['setup'] = df['setup'].apply(str)\ndf['setup'] = df['setup'].astype(int)\n\nY = df[[\"setup\",\"person\",\"time\"]]\nX = df[[\"x\",\"y\",\"z\",\"person\", \"time\"]]\n\nfacit = Y[\"setup\"][::100].values\n\n\n\n\n\nrightWrong_KNN_all = np.zeros((10,10*16))\nrightWrong_KNN_noX = np.zeros((10,10*16))\nrightWrong_KNN_noY = np.zeros((10,10*16))\nrightWrong_KNN_noZ = np.zeros((10,10*16))\n\nrightWrong_DT_all = np.zeros((10,10*16))\nrightWrong_DT_noX = np.zeros((10,10*16))\nrightWrong_DT_noY = np.zeros((10,10*16))\nrightWrong_DT_noZ = np.zeros((10,10*16))\n\nGen_error_KNN_all = 0\nGen_error_KNN_noX = 0\nGen_error_KNN_noY = 0\nGen_error_KNN_noZ = 0\n\nGen_error_DT_all = 0\nGen_error_DT_noX = 0\nGen_error_DT_noY = 0\nGen_error_DT_noZ = 0\n\nn_persons = 10\nnn = 1\n\nfor i in range(1,n_persons+1):\n    #Seperation\n    X_train = X[(X.person != i)].copy()\n    X_train['x'] = (X_train['x']-X_train['x'].mean())/X_train['x'].std()\n    X_train['y'] = (X_train['y']-X_train['y'].mean())/X_train['y'].std()\n    X_train['z'] = (X_train['z']-X_train['z'].mean())/X_train['z'].std()\n\n    X_test  = X[(X.person == i)].copy()\n    X_test['x'] = (X_test['x']-X_test['x'].mean())/X_test['x'].std()\n    X_test['y'] = (X_test['y']-X_test['y'].mean())/X_test['y'].std()\n    X_test['z'] = (X_test['z']-X_test['z'].mean())/X_test['z'].std()\n\n    Y_train = Y[(Y.person != i)].copy()\n    Y_test  = Y[(Y.person == i)].copy()\n\n    ##################################################################################################################\n    #Training site with all \n    knn_models = knn_model_train(X_train[[\"x\",\"y\",\"z\",\"time\"]],Y_train,nn)\n    DT_models = DT_model_train(X_train[[\"x\",\"y\",\"z\",\"time\"]],Y_train)\n\n    # Testing site with all\n    facit, prediction = model_test(X_test[[\"x\",\"y\",\"z\"]], Y_test, knn_models)\n    rightWrong_KNN_all[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_KNN_all += np.mean(facit==prediction)\n    print(\"KNN: Completed person {} with all coordinates with an accuracy of {}\".format(i, acc))\n\n    facit, prediction = model_test(X_test[[\"x\",\"y\",\"z\"]], Y_test, DT_models)\n    rightWrong_DT_all[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_DT_all += np.mean(facit==prediction)\n    print(\"DT: Completed person {} with all coordinates with an accuracy of {}\".format(i, acc))\n\n    ##################################################################################################################\n    #Training site with no X \n    knn_models = knn_model_train(X_train[[\"y\",\"z\",\"time\"]],Y_train,nn)\n    DT_models = DT_model_train(X_train[[\"y\",\"z\",\"time\"]],Y_train)\n\n    # Testing site with no X\n    facit, prediction = model_test(X_test[[\"y\",\"z\"]], Y_test, knn_models)\n    rightWrong_KNN_noX[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_KNN_noX += np.mean(facit==prediction)\n    print(\"KNN: Completed person {} with no X with an accuracy of {}\".format(i, acc))\n\n    facit, prediction = model_test(X_test[[\"y\",\"z\"]], Y_test, DT_models)\n    rightWrong_DT_noX[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_DT_noX += np.mean(facit==prediction)\n\n    print(\"DT: Completed person {} with no X with an accuracy of {}\".format(i, acc))\n\n    ##################################################################################################################\n    #Training site with no Z \n    knn_models = knn_model_train(X_train[[\"y\",\"x\",\"time\"]],Y_train,nn)\n    DT_models = DT_model_train(X_train[[\"y\",\"x\",\"time\"]],Y_train)\n\n    # Testing site with no Z\n    facit, prediction = model_test(X_test[[\"y\",\"x\"]], Y_test, knn_models)\n    rightWrong_KNN_noZ[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_KNN_noZ += np.mean(facit==prediction)\n    print(\"KNN: Completed person {} with no Z with an accuracy of {}\".format(i, acc))\n\n    facit, prediction = model_test(X_test[[\"y\",\"x\"]], Y_test, DT_models)\n    rightWrong_DT_noZ[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_DT_noZ += np.mean(facit==prediction)\n\n    print(\"DT: Completed person {} with no Z with an accuracy of {}\".format(i, acc))\n\n    ##################################################################################################################\n    #Training site with no Y \n    knn_models = knn_model_train(X_train[[\"z\",\"x\",\"time\"]],Y_train,nn)\n    DT_models = DT_model_train(X_train[[\"z\",\"x\",\"time\"]],Y_train)\n\n    # Testing site with no Y\n    facit, prediction = model_test(X_test[[\"z\",\"x\"]], Y_test, knn_models)\n    rightWrong_KNN_noY[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_KNN_noY += np.mean(facit==prediction)\n    print(\"KNN: Completed person {} with no Y with an accuracy of {}\".format(i, acc))\n\n    facit, prediction = model_test(X_test[[\"z\",\"x\"]], Y_test, DT_models)\n    rightWrong_DT_noY[i-1] = (facit==prediction)*1\n    acc = np.mean(facit==prediction)\n    Gen_error_DT_noY += np.mean(facit==prediction)\n\n    print(\"DT: Completed person {} with no Y with an accuracy of {}\".format(i, acc))\n#the returned generalization error when testing   (number of correctly classified setups in test)/(length of trajectories in test)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "KNN: Completed person 1 with all coordinates with an accuracy of 0.16875\nDT: Completed person 1 with all coordinates with an accuracy of 0.225\nKNN: Completed person 1 with no X with an accuracy of 0.11875\nDT: Completed person 1 with no X with an accuracy of 0.11875\nKNN: Completed person 1 with no Z with an accuracy of 0.1625\nDT: Completed person 1 with no Z with an accuracy of 0.14375\nKNN: Completed person 1 with no Y with an accuracy of 0.23125\nDT: Completed person 1 with no Y with an accuracy of 0.2375\nKNN: Completed person 2 with all coordinates with an accuracy of 0.3875\nDT: Completed person 2 with all coordinates with an accuracy of 0.41875\nKNN: Completed person 2 with no X with an accuracy of 0.30625\nDT: Completed person 2 with no X with an accuracy of 0.25\nKNN: Completed person 2 with no Z with an accuracy of 0.16875\nDT: Completed person 2 with no Z with an accuracy of 0.175\nKNN: Completed person 2 with no Y with an accuracy of 0.40625\nDT: Completed person 2 with no Y with an accuracy of 0.3625\nKNN: Completed person 3 with all coordinates with an accuracy of 0.275\nDT: Completed person 3 with all coordinates with an accuracy of 0.2875\nKNN: Completed person 3 with no X with an accuracy of 0.2125\nDT: Completed person 3 with no X with an accuracy of 0.2\nKNN: Completed person 3 with no Z with an accuracy of 0.13125\nDT: Completed person 3 with no Z with an accuracy of 0.14375\nKNN: Completed person 3 with no Y with an accuracy of 0.25\nDT: Completed person 3 with no Y with an accuracy of 0.25\nKNN: Completed person 4 with all coordinates with an accuracy of 0.33125\nDT: Completed person 4 with all coordinates with an accuracy of 0.3625\nKNN: Completed person 4 with no X with an accuracy of 0.2125\nDT: Completed person 4 with no X with an accuracy of 0.23125\nKNN: Completed person 4 with no Z with an accuracy of 0.15\nDT: Completed person 4 with no Z with an accuracy of 0.16875\nKNN: Completed person 4 with no Y with an accuracy of 0.35625\nDT: Completed person 4 with no Y with an accuracy of 0.3375\nKNN: Completed person 5 with all coordinates with an accuracy of 0.38125\nDT: Completed person 5 with all coordinates with an accuracy of 0.40625\nKNN: Completed person 5 with no X with an accuracy of 0.2625\nDT: Completed person 5 with no X with an accuracy of 0.3125\nKNN: Completed person 5 with no Z with an accuracy of 0.175\nDT: Completed person 5 with no Z with an accuracy of 0.2\nKNN: Completed person 5 with no Y with an accuracy of 0.36875\nDT: Completed person 5 with no Y with an accuracy of 0.375\nKNN: Completed person 6 with all coordinates with an accuracy of 0.36875\nDT: Completed person 6 with all coordinates with an accuracy of 0.40625\nKNN: Completed person 6 with no X with an accuracy of 0.31875\nDT: Completed person 6 with no X with an accuracy of 0.26875\nKNN: Completed person 6 with no Z with an accuracy of 0.21875\nDT: Completed person 6 with no Z with an accuracy of 0.2125\nKNN: Completed person 6 with no Y with an accuracy of 0.41875\nDT: Completed person 6 with no Y with an accuracy of 0.3875\nKNN: Completed person 7 with all coordinates with an accuracy of 0.36875\nDT: Completed person 7 with all coordinates with an accuracy of 0.35625\nKNN: Completed person 7 with no X with an accuracy of 0.30625\nDT: Completed person 7 with no X with an accuracy of 0.28125\nKNN: Completed person 7 with no Z with an accuracy of 0.23125\nDT: Completed person 7 with no Z with an accuracy of 0.20625\nKNN: Completed person 7 with no Y with an accuracy of 0.35625\nDT: Completed person 7 with no Y with an accuracy of 0.3625\nKNN: Completed person 8 with all coordinates with an accuracy of 0.3875\nDT: Completed person 8 with all coordinates with an accuracy of 0.4125\nKNN: Completed person 8 with no X with an accuracy of 0.2625\nDT: Completed person 8 with no X with an accuracy of 0.28125\nKNN: Completed person 8 with no Z with an accuracy of 0.21875\nDT: Completed person 8 with no Z with an accuracy of 0.18125\nKNN: Completed person 8 with no Y with an accuracy of 0.425\nDT: Completed person 8 with no Y with an accuracy of 0.38125\nKNN: Completed person 9 with all coordinates with an accuracy of 0.25\nDT: Completed person 9 with all coordinates with an accuracy of 0.29375\nKNN: Completed person 9 with no X with an accuracy of 0.21875\nDT: Completed person 9 with no X with an accuracy of 0.20625\nKNN: Completed person 9 with no Z with an accuracy of 0.11875\nDT: Completed person 9 with no Z with an accuracy of 0.15\nKNN: Completed person 9 with no Y with an accuracy of 0.2375\nDT: Completed person 9 with no Y with an accuracy of 0.2875\nKNN: Completed person 10 with all coordinates with an accuracy of 0.35625\nDT: Completed person 10 with all coordinates with an accuracy of 0.3875\nKNN: Completed person 10 with no X with an accuracy of 0.2625\nDT: Completed person 10 with no X with an accuracy of 0.24375\nKNN: Completed person 10 with no Z with an accuracy of 0.2125\nDT: Completed person 10 with no Z with an accuracy of 0.25\nKNN: Completed person 10 with no Y with an accuracy of 0.43125\nDT: Completed person 10 with no Y with an accuracy of 0.40625\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Gen_error_knn_noX' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f422af9270f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DT: Completed person {} with no Y with an accuracy of {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mGen_error_est_knn_noX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGen_error_knn_noX\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_persons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0mGen_error_est_knn_noY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGen_error_knn_noY\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_persons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mGen_error_est_knn_noZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGen_error_knn_noZ\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_persons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Gen_error_knn_noX' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "baf7ddb2-a7b5-463e-9af5-fec476f8a651",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "74f16d48",
    "execution_start": 1642118140457,
    "execution_millis": 185,
    "deepnote_cell_type": "code"
   },
   "source": "Gen_error_est_knn_noX = 1-(Gen_error_KNN_noX / n_persons)\nGen_error_est_knn_noY = 1-(Gen_error_KNN_noY / n_persons)\nGen_error_est_knn_noZ = 1-(Gen_error_KNN_noZ / n_persons)\nGen_error_est_knn_all = 1-(Gen_error_KNN_all / n_persons)\n\nGen_error_est_DT_noX  = 1-(Gen_error_DT_noX / n_persons)\nGen_error_est_DT_noY  = 1-(Gen_error_DT_noY / n_persons)\nGen_error_est_DT_noZ  = 1-(Gen_error_DT_noZ / n_persons)\nGen_error_est_DT_all  = 1-(Gen_error_DT_all / n_persons)\n\nprint(\"Completed {} number of neighbours with all coordinates with an estimated generalization error of {}.\".format(nn, Gen_error_est_knn_all))\nprint(\"Completed {} number of neighbours with no X with an estimated generalization error of {}.\".format(nn, Gen_error_est_knn_noX))\nprint(\"Completed {} number of neighbours with no Y with an estimated generalization error of {}.\".format(nn, Gen_error_est_knn_noY))\nprint(\"Completed {} number of neighbours with no Z with an estimated generalization error of {}.\".format(nn, Gen_error_est_knn_noZ))\n\nprint(\"Completed Decision Tree with all coordinates an estimated generalization error of {}.\".format(Gen_error_est_DT_all))\nprint(\"Completed Decision Tree with no X an estimated generalization error of {}.\".format(Gen_error_est_DT_noX))\nprint(\"Completed Decision Tree with no Y an estimated generalization error of {}.\".format(Gen_error_est_DT_noY))\nprint(\"Completed Decision Tree with no Z an estimated generalization error of {}.\".format(Gen_error_est_DT_noZ))\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Completed 1 number of neighbours with all coordinates with an estimated generalization error of 0.6725.\nCompleted 1 number of neighbours with no X with an estimated generalization error of 0.751875.\nCompleted 1 number of neighbours with no Y with an estimated generalization error of 0.651875.\nCompleted 1 number of neighbours with no Z with an estimated generalization error of 0.82125.\nCompleted Decision Tree with all coordinates an estimated generalization error of 0.6443749999999999.\nCompleted Decision Tree with no X an estimated generalization error of 0.760625.\nCompleted Decision Tree with no Y an estimated generalization error of 0.66125.\nCompleted Decision Tree with no Z an estimated generalization error of 0.816875.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "49af7917-24dd-4594-bb4d-a958349ecece",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "74511c76",
    "execution_start": 1642118160489,
    "execution_millis": 1111,
    "deepnote_cell_type": "code"
   },
   "source": "# df = pd.DataFrame (rightWrong_KNN_all)\n# filepath = 'rightWrong_KNN_all.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_KNN_noX)\n# filepath = 'rightWrong_KNN_noX.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_KNN_noY)\n# filepath = 'rightWrong_KNN_noY.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_KNN_noZ)\n# filepath = 'rightWrong_KNN_noZ.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_KNN_noX)\n# filepath = 'rightWrong_KNN_noX.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_DT_all)\n# filepath = 'rightWrong_DT_all.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_DT_noX)\n# filepath = 'rightWrong_DT_noX.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_DT_noY)\n# filepath = 'rightWrong_DT_noY.xlsx'\n# df.to_excel(filepath, index=False)\n\n# df = pd.DataFrame (rightWrong_DT_noZ)\n# filepath = 'rightWrong_DT_noZ.xlsx'\n# df.to_excel(filepath, index=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## McNemar",
   "metadata": {
    "cell_id": "6a72713e-ab08-4e2c-a88b-ae5436976634",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "2c22636b-07fa-41e3-aec7-53a983accfe9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c725c8f8",
    "execution_start": 1642159792283,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np; np.random.seed(0)\nimport seaborn as sns; sns.set_theme()\nfrom scipy.stats import binom",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "253e1b4c-0ca1-4b15-837a-2cc457353f5e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d01b41c",
    "execution_start": 1642159970863,
    "execution_millis": 925,
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np; np.random.seed(0)\nimport seaborn as sns; sns.set_theme()\nimport scipy.stats\nimport numpy as np\nimport scipy.stats as st\n\nmodels = []\nmodel = pd.read_excel(\"rightWrong_DT_all.xlsx\")\nmodel_DT_ALL= np.array(model)\nmodels.append(model_DT_ALL)\n\nmodel = pd.read_excel(\"rightWrong_DT_noX.xlsx\")\nmodel_DT_noX= np.array(model)\nmodels.append(model_DT_noX)\n\nmodel = pd.read_excel(\"rightWrong_DT_noY.xlsx\")\nmodel_DT_noY= np.array(model)\nmodels.append(model_DT_noY)\n\nmodel = pd.read_excel(\"rightWrong_DT_noZ.xlsx\")\nmodel_DT_noZ= np.array(model)\nmodels.append(model_DT_noZ)\n\nmodel = pd.read_excel(\"rightWrong_KNN_all.xlsx\")\nmodel_KNN_all= np.array(model)\nmodels.append(model_KNN_all)\n\nmodel = pd.read_excel(\"rightWrong_KNN_noX.xlsx\")\nmodel_KNN_noX= np.array(model)\nmodels.append(model_KNN_noX)\n\nmodel = pd.read_excel(\"rightWrong_KNN_noY.xlsx\")\nmodel_KNN_noY= np.array(model)\nmodels.append(model_KNN_noY)\n\nmodel = pd.read_excel(\"rightWrong_KNN_noZ.xlsx\")\nmodel_KNN_noZ= np.array(model)\nmodels.append(model_KNN_noZ)\n\nn = 1600\nalpha = 0.05\n\ndef mcnemar(results1, results2, n, alpha):\n    n12 = ((results1-results2)==-1).sum()\n    n21 = ((results1-results2)==1).sum()\n\n\n    thetahat = (n12-n21)/n\n    Etheta = thetahat\n\n    Q = n**2 * (n+1) * (Etheta+1) * (1-Etheta) / ( (n*(n12+n21) - (n12-n21)**2) )\n\n    p = (Etheta + 1)*0.5 * (Q-1)\n    q = (1-Etheta)*0.5 * (Q-1)\n\n    # CI = tuple(lm * 2 - 1 for lm in scipy.stats.beta.interval(1-alpha, a=p, b=q) )\n\n    p = 2*scipy.stats.binom.cdf(min([n12,n21]), n=n12+n21, p=0.5)\n\n    return p\n\n    # print(\"Result of McNemars test using alpha=\", alpha)\n    # print(\"Comparison matrix n\")\n    # if n12+n21 <= 10:\n    #     print(\"Warning, n12+n21 is low: n12+n21=\",(n12+n21))\n\n    # print(\"Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = \", CI)\n    # print(\"p-value for two-sided test A and B have same accuracy (exact binomial test): p=\", p)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 17,
     "data": {
      "text/plain": "2.2347052835977196e-13"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9cabbf1f-ad60-47b3-af3c-cdf3732148b9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "ea605395-3428-4db4-a0fb-cca2fd742187",
  "deepnote_execution_queue": []
 }
}